<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Ray Bernard's Fine-Tune Journal</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.
I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace."><meta name=generator content="Hugo 0.126.2"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link rel=canonical href=https://raymondbernard.github.io/posts/llama3-updated/><meta property="og:url" content="https://raymondbernard.github.io/posts/llama3-updated/"><meta property="og:site_name" content="Ray Bernard's Fine-Tune Journal"><meta property="og:title" content="Ray Bernard's Fine-Tune Journal"><meta property="og:description" content="Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.
I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta itemprop=name content="Ray Bernard's Fine-Tune Journal"><meta itemprop=description content="Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.
I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace."><meta itemprop=wordCount content="5471"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ray Bernard's Fine-Tune Journal"><meta name=twitter:description content="Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.
I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace."><script>(function(e,t,n,s,o,i){e.hj=e.hj||function(){(e.hj.q=e.hj.q||[]).push(arguments)},e._hjSettings={hjid:5009664,hjsv:6},o=t.getElementsByTagName("head")[0],i=t.createElement("script"),i.async=1,i.src=n+e._hjSettings.hjid+s+e._hjSettings.hjsv,o.appendChild(i)})(window,document,"https://static.hotjar.com/c/hotjar-",".js?sv=")</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-XT709SJYMZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XT709SJYMZ")</script></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Ray Bernard's Fine-Tune Journal</a><div class="flex-l items-center"><div class=ananke-socials><a href=https://www.github.com/raymondbernard/fine-tune-journal target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="github link" aria-label="follow on github——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.linkedin.com/in/raymond-bernard-960382/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="My Linkedin link" aria-label="follow on My Linkedin——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q target=_blank rel=noopener class="youtube ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="youtube link" aria-label="follow on youtube——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527 41.34c-.278.0-.478.078-.6.244-.121.156-.18.424-.18.796v.896h1.543V42.38c0-.372-.062-.64-.185-.796C42.989 41.418 42.792 41.34 42.527 41.34zM36.509 41.309c.234.0.417.076.544.23.123.155.185.383.185.682v4.584c0 .286-.053.487-.153.611-.1.127-.256.189-.47.189-.148.0-.287-.033-.421-.096-.135-.062-.274-.171-.415-.313v-5.531c.119-.122.239-.213.36-.271C36.26 41.335 36.383 41.309 36.509 41.309zm5.239 3.349v1.672c0 .468.057.792.17.974.118.181.313.269.592.269.289.0.491-.076.606-.229.114-.153.175-.489.175-1.013v-.405h1.795v.456c0 .911-.217 1.596-.657 2.059-.435.459-1.089.687-1.958.687-.781.0-1.398-.242-1.847-.731-.448-.486-.676-1.157-.676-2.014v-3.986c0-.768.249-1.398.742-1.882.493-.484 1.128-.727 1.911-.727.799.0 1.413.225 1.843.674.429.448.642 1.093.642 1.935v2.264H41.748zm-3.125 3.837c-.271.336-.669.501-1.187.501-.343.0-.646-.062-.912-.192-.267-.129-.519-.327-.746-.601v.681h-1.764V36.852h1.764v3.875c.237-.27.485-.478.748-.616.267-.143.534-.212.805-.212.554.0.975.189 1.265.565.294.379.438.933.438 1.66v4.926C39.034 47.678 38.897 48.159 38.623 48.495zM30.958 48.884v-.976c-.325.361-.658.636-1.009.822-.349.191-.686.282-1.014.282-.405.0-.705-.129-.913-.396-.201-.266-.305-.658-.305-1.189v-7.422h1.744v6.809c0 .211.037.362.107.457.077.095.196.141.358.141.128.0.292-.062.488-.188.197-.125.375-.283.542-.475v-6.744H32.7v8.878H30.958zM24.916 38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916zm8.078-5.622c0-.001 12.08.018 13.514 1.45 1.439 1.435 1.455 8.514 1.455 8.555.0.0-.012 7.117-1.455 8.556C45.074 52.969 32.994 53 32.994 53s-12.079-.031-13.516-1.462c-1.438-1.435-1.441-8.502-1.441-8.556.0-.041.004-7.12 1.441-8.555 1.438-1.431 13.516-1.45 13.516-1.449zm9.526-3.723h-1.966v-1.08c-.358.397-.736.703-1.13.909-.392.208-.771.312-1.14.312-.458.0-.797-.146-1.027-.437-.229-.291-.345-.727-.345-1.311v-8.172h1.962v7.497c0 .231.045.399.127.502.08.104.216.156.399.156.143.0.327-.069.548-.206.22-.137.423-.312.605-.527v-7.422h1.966V29.255zM31.847 27.588c.139.147.339.219.6.219.266.0.476-.075.634-.223.157-.152.235-.358.235-.618v-5.327c0-.214-.08-.387-.241-.519-.16-.131-.37-.196-.628-.196-.241.0-.435.065-.586.196-.148.132-.225.305-.225.519v5.327C31.636 27.233 31.708 27.439 31.847 27.588zm-1.439-7.685c.528-.449 1.241-.674 2.132-.674.812.0 1.48.237 2.001.711.517.473.777 1.083.777 1.828v5.051c0 .836-.255 1.491-.762 1.968-.513.476-1.212.714-2.106.714-.858.0-1.547-.246-2.064-.736-.513-.492-.772-1.152-.772-1.983v-5.068C29.613 20.954 29.877 20.351 30.408 19.903zM24.262 16h-2.229l2.634 8.003v5.252h2.213v-5.5L29.454 16h-2.25l-1.366 5.298h-.139L24.262 16zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://x.com/raybernard007 target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://medium.com/@raybernard007 target=_blank rel=noopener class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 170 170" viewBox="0 0 170 170" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M46.5340803 65.2157554C46.6968378 63.6076572 46.0836 62.018231 44.8828198 60.93592L32.6512605 46.2010582V44H70.6302521l29.3557423 64.380952L125.794585 44H162v2.2010582L151.542017 56.2281011C150.640424 56.9153477 150.193188 58.0448862 150.380019 59.1628454V132.837155C150.193188 133.955114 150.640424 135.084652 151.542017 135.771899l10.213352 10.027043V148H110.38282V145.798942l10.580299-10.271605c1.039682-1.039389 1.039682-1.345091 1.039682-2.934744V73.0417402l-29.4169 74.7136978H88.6106443L54.3622782 73.0417402V123.115814C54.0767278 125.221069 54.7759199 127.3406 56.2581699 128.863022L70.0186741 145.55438V147.755438H31V145.55438l13.7605042-16.691358c1.4714579-1.524946 2.1298796-3.658537 1.7735761-5.747208V65.2157554z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.reddit.com/user/OpenAITutor/ target=_blank rel=noopener class="reddit ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="reddit link" aria-label="follow on reddit——Opens in a new window">reddit
<span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://raymondbernard.github.io/posts/llama3-updated/&amp;title=" class="ananke-social-link linkedin no-underline" aria-label="share on My Linkedin"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href="https://twitter.com/intent/tweet?url=https://raymondbernard.github.io/posts/llama3-updated/&amp;text=" class="ananke-social-link twitter no-underline" aria-label="share on twitter"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div><h1 class="f1 athelas mt3 mb1"></h1><span class="f6 mv4 dib tracked">- 26 minutes read </span><span class="f6 mv4 dib tracked">- 5471 words</span></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><img src=/images/llama3.png alt="LLaMA 3"></p><h2 id=fine-tuning-llama-3-8b-instruct-on-intel-max-series-gpus-an-exciting-journey>Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey</h2><p>In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.</p><p>I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace. By importing my dataset, obtaining the LLaMa 3 8B Instruct model, and training it using the Transformer Trainer, I was able to achieve outstanding results.</p><p>The entire training process was meticulously monitored by Weights and Biases, providing detailed insights into memory usage, disk I/O, training loss, and more. It&rsquo;s truly an outstanding product!</p><p>The best part? It&rsquo;s all free! I was amazed by the capabilities of the Intel Developer Cloud, particularly for machine learning (ML), high-performance computing (HPC), and generative AI (GenAI) workflows. The Intel® Data Center Max 1100 GPU, a high-performance 300-watt double-wide AIC card, features 56 Xe cores and 48 GB of HBM2E memory, delivering exceptional performance.</p><p>In the spirit of open source, I developed the following notebook, partially based on the original work of Rahul Unnikrishnan Nair from Intel, &ldquo;Fine-tuning Google&rsquo;s Gemma Model on Intel Max Series GPUs.&rdquo; Thank you for the foundation. I was able to significantly tweak and create a process to fine-tune LLaMa 3 models, which represent a significant leap from LLaMa 2 in terms of capabilities. It&rsquo;s amazing how such small models can produce great results with quality data.</p><p>I will be producing a detailed video review of the notebook, which you can find on my YouTube channel: <a href=https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q>YouTube Channel</a>.</p><p><strong>Note: This code was executed in a Jupyter notebook on Intel&rsquo;s Developer Cloud.</strong></p><h1 id=table-of-contents>Table of Contents</h1><ul><li><a href=#fine-tuning-llama-3-8b-instruct-on-intel-max-series-gpus>Fine-tuning LLaMa 3 8B Instruct on Intel Max Series GPUs</a><ul><li><a href=#step-1-initial-setup>Step 1: Initial Setup</a></li><li><a href=#step-2-check-intel-xpu-availability-and-retrieve-device-capabilities>Step 2: Check Intel XPU Availability and Retrieve Device Capabilities</a></li><li><a href=#step-3-optimize-environment-for-intel-max-series-gpus>Step 3: Optimize Environment for Intel Max Series GPUs</a></li><li><a href=#step-4-monitor-xpu-memory-usage-in-real-time>Step 4: Monitor XPU Memory Usage in Real-Time</a></li><li><a href=#printxpu-device-not-available>print(&ldquo;XPU device not available.&rdquo;)</a></li><li><a href=#step-5-log-into-your-hugging-face-account-with-your-access-token>Step 5: Log into Your Hugging Face Account with Your Access Token</a></li><li><a href=#step-6-configure-lora-for-efficient-training->Step 6: Configure LoRA for Efficient Training 🎛️</a></li><li><a href=#step-7-load-and-prepare-the-model>Step 7: Load and Prepare the Model</a></li><li><a href=#step-8-testing-the-model>Step 8: Testing the Model</a></li><li><a href=#step-9-load-and-inspect-the-dataset>Step 9: Load and Inspect the Dataset</a></li><li><a href=#step-10-format-and-split-the-dataset-for-training>Step 10: Format and Split the Dataset for Training</a></li><li><a href=#step-11-fine-tune-the-model-and-save-the-results->Step 11: Fine-Tune the Model and Save the Results 📊</a></li><li><a href=#step-12-merge-and-save-the-fine-tuned-model>Step 12: Merge and Save the Fine-Tuned Model</a></li><li><a href=#step-13-upload-the-fine-tuned-model-to-hugging-face-hub->Step 13: Upload the Fine-Tuned Model to Hugging Face Hub 🚀</a></li><li><a href=#step-14-fine-tuning-results-and-observations>Step 14: Fine-Tuning Results and Observations</a></li><li><a href=#happy-fine-tuning->Happy Fine-Tuning! 😄✨</a></li></ul></li></ul><h3 id=step-1-initial-setup>Step 1: Initial Setup</h3><p>Before we begin the fine-tuning process, it&rsquo;s essential to ensure that we have the proper libraries installed and the correct kernel configured. This step needs to be performed only once.
First, make sure you are using the Modin kernel on the Intel Developer Cloud. The Modin kernel is a specialized kernel designed for efficient data processing and analysis. To access the Modin kernel, follow these steps:
Join the Intel Developer Cloud by creating an account.
Once logged in, navigate to the &ldquo;Free Training&rdquo; section.
You will be presented with a Jupyter Lab environment, where you can select the Modin kernel.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> site
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> subprocess
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> shutil
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Uninstall the invalid distributions if they are partially installed</span>
</span></span><span style=display:flex><span>subprocess<span style=color:#f92672>.</span>run([sys<span style=color:#f92672>.</span>executable, <span style=color:#e6db74>&#34;-m&#34;</span>, <span style=color:#e6db74>&#34;pip&#34;</span>, <span style=color:#e6db74>&#34;uninstall&#34;</span>, <span style=color:#e6db74>&#34;-y&#34;</span>, <span style=color:#e6db74>&#34;torch&#34;</span>, <span style=color:#e6db74>&#34;transformers&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Clean up the site-packages directory</span>
</span></span><span style=display:flex><span>site_packages_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>expanduser(<span style=color:#e6db74>&#34;~/.local/lib/python3.9/site-packages&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>remove_directory</span>(dir_path):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(dir_path):
</span></span><span style=display:flex><span>        shutil<span style=color:#f92672>.</span>rmtree(dir_path, ignore_errors<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch_dirs <span style=color:#f92672>=</span> [os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(site_packages_dir, d) <span style=color:#66d9ef>for</span> d <span style=color:#f92672>in</span> os<span style=color:#f92672>.</span>listdir(site_packages_dir) <span style=color:#66d9ef>if</span> d<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#34;torch&#34;</span>)]
</span></span><span style=display:flex><span>transformers_dirs <span style=color:#f92672>=</span> [os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(site_packages_dir, d) <span style=color:#66d9ef>for</span> d <span style=color:#f92672>in</span> os<span style=color:#f92672>.</span>listdir(site_packages_dir) <span style=color:#66d9ef>if</span> d<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#34;transformers&#34;</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> dir_path <span style=color:#f92672>in</span> torch_dirs <span style=color:#f92672>+</span> transformers_dirs:
</span></span><span style=display:flex><span>    remove_directory(dir_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check the size of the cache directory</span>
</span></span><span style=display:flex><span>cache_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>expanduser(<span style=color:#e6db74>&#34;~/.cache&#34;</span>)
</span></span><span style=display:flex><span>result <span style=color:#f92672>=</span> subprocess<span style=color:#f92672>.</span>run([<span style=color:#e6db74>&#34;du&#34;</span>, <span style=color:#e6db74>&#34;-sh&#34;</span>, cache_dir], capture_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, text<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;disk space used by cache: </span><span style=color:#e6db74>{</span>result<span style=color:#f92672>.</span>stdout<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Install the required packages in the correct order</span>
</span></span><span style=display:flex><span><span style=color:#75715e># First, install torch and related packages with the extra index URL</span>
</span></span><span style=display:flex><span>torch_packages <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;torch==2.1.0.post2&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;torchvision==0.16.0.post2&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;torchaudio==2.1.0.post2&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;intel-extension-for-pytorch==2.1.30+xpu&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;oneccl_bind_pt==2.1.300+xpu&#34;</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>extra_index_url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;--extra-index-url=https://pytorch-extension.intel.com/release-whl/stable/xpu/us/&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> package <span style=color:#f92672>in</span> torch_packages:
</span></span><span style=display:flex><span>    subprocess<span style=color:#f92672>.</span>run([sys<span style=color:#f92672>.</span>executable, <span style=color:#e6db74>&#34;-m&#34;</span>, <span style=color:#e6db74>&#34;pip&#34;</span>, <span style=color:#e6db74>&#34;install&#34;</span>, <span style=color:#e6db74>&#34;--upgrade&#34;</span>, package, extra_index_url])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Then, install the other dependencies</span>
</span></span><span style=display:flex><span>other_packages <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;transformers&gt;=4.38.*&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;datasets&gt;=2.18.*&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;wandb&gt;=0.16.*&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;trl&gt;=0.7.11&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;peft&gt;=0.9.0&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;accelerate&gt;=0.28.*&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;joblib&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;threadpoolctl&#34;</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> package <span style=color:#f92672>in</span> other_packages:
</span></span><span style=display:flex><span>    subprocess<span style=color:#f92672>.</span>run([sys<span style=color:#f92672>.</span>executable, <span style=color:#e6db74>&#34;-m&#34;</span>, <span style=color:#e6db74>&#34;pip&#34;</span>, <span style=color:#e6db74>&#34;install&#34;</span>, <span style=color:#e6db74>&#34;--upgrade&#34;</span>, package])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Get the site-packages directory</span>
</span></span><span style=display:flex><span>site_packages_dir <span style=color:#f92672>=</span> site<span style=color:#f92672>.</span>getsitepackages()[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add the site-packages directory where these packages are installed to the top of sys.path</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>access(site_packages_dir, os<span style=color:#f92672>.</span>W_OK):
</span></span><span style=display:flex><span>    user_site_packages_dir <span style=color:#f92672>=</span> site<span style=color:#f92672>.</span>getusersitepackages()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> user_site_packages_dir <span style=color:#f92672>in</span> sys<span style=color:#f92672>.</span>path:
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>remove(user_site_packages_dir)
</span></span><span style=display:flex><span>    sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, user_site_packages_dir)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> site_packages_dir <span style=color:#f92672>in</span> sys<span style=color:#f92672>.</span>path:
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>remove(site_packages_dir)
</span></span><span style=display:flex><span>    sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, site_packages_dir)
</span></span></code></pre></div><h3 id=step-2-check-intel-xpu-availability-and-retrieve-device-capabilities>Step 2: Check Intel XPU Availability and Retrieve Device Capabilities</h3><p>In this step, we will import necessary libraries, check the availability of Intel XPU (eXtreme Performance Unit), and retrieve detailed device capabilities. This ensures that our environment is correctly configured to leverage the Intel XPU for optimal performance available.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> intel_extension_for_pytorch <span style=color:#66d9ef>as</span> ipex
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check if Intel XPU is available</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Intel XPU is available&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>device_count()):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU Device </span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_name(i)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Get the device capability details</span>
</span></span><span style=display:flex><span>    device_capability <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_capability()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Convert the device capability details to a JSON string with indentation for readability</span>
</span></span><span style=display:flex><span>    readable_device_capability <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>dumps(device_capability, indent<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print the readable JSON</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Detail of GPU capability =</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, readable_device_capability)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Intel XPU is not available&#34;</span>)
</span></span></code></pre></div><h3 id=step-3-optimize-environment-for-intel-max-series-gpus>Step 3: Optimize Environment for Intel Max Series GPUs</h3><p>To optimize performance when using Intel Max Series GPUs:</p><ol><li><strong>Suppress Warnings</strong>: Import the <code>warnings</code> module and configure it to ignore unnecessary warnings.</li><li><strong>Import Required Modules</strong>: Use the <code>os</code> and <code>psutil</code> modules for setting environment variables and retrieving CPU information.</li><li><strong>Retrieve CPU Information</strong>: Determine the number of physical CPU cores and calculate cores per socket using <code>psutil</code>.</li><li><strong>Set Environment Variables</strong>:<ul><li>Disable tokenizers parallelism.</li><li>Improve memory allocation with <code>LD_PRELOAD</code> (optional).</li><li>Reduce GPU command submission overhead.</li><li>Enable SDP fusion for efficient memory usage.</li><li>Configure OpenMP to use physical cores, bind threads, and set thread pinning.</li></ul></li><li><strong>Print Configuration</strong>: Display the number of physical cores, cores per socket, and OpenMP environment variables to verify the settings.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> warnings
</span></span><span style=display:flex><span>warnings<span style=color:#f92672>.</span>filterwarnings(<span style=color:#e6db74>&#34;ignore&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> psutil
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_physical_cores <span style=color:#f92672>=</span> psutil<span style=color:#f92672>.</span>cpu_count(logical<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>num_cores_per_socket <span style=color:#f92672>=</span> num_physical_cores <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;TOKENIZERS_PARALLELISM&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#HF_TOKEN = os.environ[&#34;HF_TOKEN&#34;]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set the LD_PRELOAD environment variable</span>
</span></span><span style=display:flex><span>ld_preload <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;LD_PRELOAD&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>conda_prefix <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;CONDA_PREFIX&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Improve memory allocation performance, if tcmalloc is not available, please comment this line out</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;LD_PRELOAD&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>ld_preload<span style=color:#e6db74>}</span><span style=color:#e6db74>:</span><span style=color:#e6db74>{</span>conda_prefix<span style=color:#e6db74>}</span><span style=color:#e6db74>/lib/libtcmalloc.so&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Reduce the overhead of submitting commands to the GPU</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># reducing memory accesses by fusing SDP ops</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;ENABLE_SDP_FUSION&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># set openMP threads to number of physical cores</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;OMP_NUM_THREADS&#34;</span>] <span style=color:#f92672>=</span> str(num_physical_cores)
</span></span><span style=display:flex><span><span style=color:#75715e># Set the thread affinity policy</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;OMP_PROC_BIND&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;close&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set the places for thread pinning</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;OMP_PLACES&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;cores&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of physical cores: </span><span style=color:#e6db74>{</span>num_physical_cores<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of cores per socket: </span><span style=color:#e6db74>{</span>num_cores_per_socket<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;OpenMP environment variables:&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;  - OMP_NUM_THREADS: </span><span style=color:#e6db74>{</span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;OMP_NUM_THREADS&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;  - OMP_PROC_BIND: </span><span style=color:#e6db74>{</span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;OMP_PROC_BIND&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;  - OMP_PLACES: </span><span style=color:#e6db74>{</span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;OMP_PLACES&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><h3 id=step-4-monitor-xpu-memory-usage-in-real-time>Step 4: Monitor XPU Memory Usage in Real-Time</h3><p>The following script sets up a real-time monitoring system that continuously displays the XPU memory usage in a Jupyter notebook, helping you keep track of resource utilization during model training and inference. This setup helps in maintaining optimal performance and preventing resource-related issues during your deep learning tasks. By keeping track of memory usage, you can prevent out-of-memory errors, optimize resource allocation, and ensure smooth training and inference processes. By monitoring these metrics, you can predict out-of-memory issues. If memory usage approaches the hardware limits, it’s an indication that the model or batch size might need adjusted etc.</p><ul><li><strong>Memory Reserved</strong>: Indicates the total memory reserved by the XPU. Helps in understanding the memory footprint of the running processes.</li><li><strong>Memory Allocated</strong>: Shows the actual memory usage by tensors, crucial for identifying memory leaks or excessive usage.</li><li><strong>Max Memory Reserved/Allocated</strong>: These metrics help in identifying peak memory usage, which is essential for planning and scaling your models.</li><li>performance and preventing resource-related issues during your deep learning tasks.eemory_monitor(output)</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> asyncio
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> threading
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> IPython.display <span style=color:#f92672>import</span> display, HTML
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> intel_extension_for_pytorch <span style=color:#66d9ef>as</span> ipex
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available():
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>empty_cache()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_memory_usage</span>():
</span></span><span style=display:flex><span>        memory_reserved <span style=color:#f92672>=</span> round(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>memory_reserved() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span><span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        memory_allocated <span style=color:#f92672>=</span> round(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>memory_allocated() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span><span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        max_memory_reserved <span style=color:#f92672>=</span> round(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>max_memory_reserved() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span><span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        max_memory_allocated <span style=color:#f92672>=</span> round(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>max_memory_allocated() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span><span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>print_memory_usage</span>():
</span></span><span style=display:flex><span>        device_name <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_name()
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU Name: </span><span style=color:#e6db74>{</span>device_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated <span style=color:#f92672>=</span> get_memory_usage()
</span></span><span style=display:flex><span>        memory_usage_text <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU Memory: Reserved=</span><span style=color:#e6db74>{</span>memory_reserved<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Allocated=</span><span style=color:#e6db74>{</span>memory_allocated<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Max Reserved=</span><span style=color:#e6db74>{</span>max_memory_reserved<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Max Allocated=</span><span style=color:#e6db74>{</span>max_memory_allocated<span style=color:#e6db74>}</span><span style=color:#e6db74> GB&#34;</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\r</span><span style=color:#e6db74>{</span>memory_usage_text<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span>, flush<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>display_memory_usage</span>(output):
</span></span><span style=display:flex><span>        device_name <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_name()
</span></span><span style=display:flex><span>        output<span style=color:#f92672>.</span>update(HTML(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&lt;p&gt;XPU Name: </span><span style=color:#e6db74>{</span>device_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&lt;/p&gt;&#34;</span>))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated <span style=color:#f92672>=</span> get_memory_usage()
</span></span><span style=display:flex><span>            memory_usage_text <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU (</span><span style=color:#e6db74>{</span>device_name<span style=color:#e6db74>}</span><span style=color:#e6db74>) :: Memory: Reserved=</span><span style=color:#e6db74>{</span>memory_reserved<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Allocated=</span><span style=color:#e6db74>{</span>memory_allocated<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Max Reserved=</span><span style=color:#e6db74>{</span>max_memory_reserved<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Max Allocated=</span><span style=color:#e6db74>{</span>max_memory_allocated<span style=color:#e6db74>}</span><span style=color:#e6db74> GB&#34;</span>
</span></span><span style=display:flex><span>            output<span style=color:#f92672>.</span>update(HTML(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&lt;p&gt;</span><span style=color:#e6db74>{</span>memory_usage_text<span style=color:#e6db74>}</span><span style=color:#e6db74>&lt;/p&gt;&#34;</span>))
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>start_memory_monitor</span>(output):
</span></span><span style=display:flex><span>        loop <span style=color:#f92672>=</span> asyncio<span style=color:#f92672>.</span>new_event_loop()
</span></span><span style=display:flex><span>        asyncio<span style=color:#f92672>.</span>set_event_loop(loop)
</span></span><span style=display:flex><span>        loop<span style=color:#f92672>.</span>create_task(display_memory_usage(output))
</span></span><span style=display:flex><span>        thread <span style=color:#f92672>=</span> threading<span style=color:#f92672>.</span>Thread(target<span style=color:#f92672>=</span>loop<span style=color:#f92672>.</span>run_forever)
</span></span><span style=display:flex><span>        thread<span style=color:#f92672>.</span>start()    
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> display(display_id<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    start_memory_monitor(output)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;XPU device not available.&#34;</span>)
</span></span></code></pre></div><h2 id=step-5-log-into-your-hugging-face-account-with--your-access-token>Step 5 Log into your hugging face account with your access token.</h2><p>Uncheck the Add token as git credential! 🎛️</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#loggin to huggnigface</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> huggingface_hub <span style=color:#f92672>import</span> notebook_login
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>notebook_login()
</span></span></code></pre></div><pre><code>VBox(children=(HTML(value='&lt;center&gt; &lt;img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…
</code></pre><h2 id=step-6-configure-lora-for-efficient-training->Step 6 Configure LoRA for Efficient Training 🎛️</h2><p>In this step, we configure the LoRA (Low-Rank Adaptation) settings for efficient training of our model. LoRA is a technique that improves the efficiency of training by reducing the number of parameters through low-rank decomposition. Here, we instantiate a LoraConfig object with specific parameters tailored to our training needs.</p><p>Instantiate LoRA Configuration:</p><ul><li>r: Set to 32, this parameter controls the dimension of the low-rank decomposition, balancing model capacity and efficiency.</li><li>lora_alpha: Set to 16, this scaling factor adjusts the output of the low-rank decomposition, influencing the strength of the adaptation.</li><li>lora_dropout: Set to 0.5, this dropout rate applies regularization to the LoRA layers to prevent overfitting. A higher value increases regularization.</li><li>bias: Set to &ldquo;none&rdquo;, indicating no bias is added to the LoRA layers.</li><li>target_modules: Specifies the layers where the low-rank adaptation will be applied. Here, it includes &ldquo;q_proj&rdquo;, &ldquo;k_proj&rdquo;, &ldquo;v_proj&rdquo;, and &ldquo;output_proj&rdquo;.</li><li>task_type: Set to &ldquo;CAUSAL_LM&rdquo;, indicating that this configuration is for a causal language modeling task.</li><li>This configuration optimizes the model&rsquo;s training efficiency and performance by carefully adjusting the parameters and specifying the target modules for low-rank adaptation.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> LoraConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Instantiate a LoraConfig object with specific parameters</span>
</span></span><span style=display:flex><span>lora_config <span style=color:#f92672>=</span> LoraConfig(
</span></span><span style=display:flex><span>    r<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>,  <span style=color:#75715e># The dimension of the low-rank decomposition. This parameter controls the trade-off between model capacity and efficiency.</span>
</span></span><span style=display:flex><span>    lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>,  <span style=color:#75715e># The scaling factor for the LoRA module. It is used to adjust the output of the low-rank decomposition.</span>
</span></span><span style=display:flex><span>    lora_dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>,  <span style=color:#75715e># The dropout rate applied to the LoRA layers to prevent overfitting. A higher value means more regularization.</span>
</span></span><span style=display:flex><span>    bias<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;none&#34;</span>,  <span style=color:#75715e># Specifies how to handle biases in the LoRA layers. &#34;none&#34; means no bias is added.</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># The target modules for the LoRA transformation. These are the specific layers in the model where the low-rank adaptation will be applied.</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># You could use &#39;q_proj&#39;, &#39;v_proj&#39;, and &#39;0_proj&#39; as well and comment out the rest if needed.</span>
</span></span><span style=display:flex><span>    target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;q_proj&#34;</span>, <span style=color:#e6db74>&#34;k_proj&#34;</span>, <span style=color:#e6db74>&#34;v_proj&#34;</span>, <span style=color:#e6db74>&#34;output_proj&#34;</span>],  
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    task_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;CAUSAL_LM&#34;</span>  <span style=color:#75715e># Specifies the task type for which this configuration is being used. &#34;CAUSAL_LM&#34; stands for causal language modeling.</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=step-7-load-and-prepare-the-model>Step 7: Load and Prepare the Model</h3><p>In this step, we ensure the model is loaded and prepared for use on the appropriate device, either an Intel XPU or CPU, and configure it for efficient fine-tuningThis ensures the model and tokenizer are properly set up and optimized for use on the selected device, ready for efficient fine-tuning.
This step ensures that the model and tokenizer are correctly set up and configured for use on the appropriate device, preparing them for the fine-tuning process.
.</p><ol><li><p><strong>Check Device Availability</strong>:</p><ul><li>Check if an XPU is available and set the device accordingly. If the XPU is available and <code>USE_CPU</code> is not set to <code>True</code>, use the XPU; otherwise, use the CPU.</li></ul></li><li><p><strong>Specify Model Name</strong>:</p><ul><li>Define the model name to be used.</li></ul></li><li><p><strong>Download Model if Not Existing Locally</strong>:</p><ul><li>Define a function to check if the model exists locally.</li><li>If the model does not exist locally, download it from the specified model name, save the tokenizer and model locally.</li></ul></li><li><p><strong>Load Model and Tokenizer</strong>:</p><ul><li>Load the model and tokenizer from the local directory where they were saved.</li><li>Set the padding token and padding side for the tokenizer.</li><li>Resize the model&rsquo;s embeddings to account for any new special tokens added.</li><li>Set the padding token ID in the model&rsquo;s generation configuration.</li></ul></li><li><p><strong>Move Model to Device</strong>:</p><ul><li>Move the model to the appropriate device (XPU or CPU).</li></ul></li><li><p><strong>Configure Model for Fine-Tuning</strong>:</p><ul><li>Disable the caching mechanism to reduce memory usage during fine-tuning.</li><li>Configure the model&rsquo;s pre-training teigured for use on the appropriate device, preparing them for the fine-tuning process.</li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check if XPU is available and set the device accordingly</span>
</span></span><span style=display:flex><span>USE_CPU <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;xpu:0&#34;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available() <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> USE_CPU <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Using device: </span><span style=color:#e6db74>{</span>device<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Specify the model name</span>
</span></span><span style=display:flex><span>model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define a function to check if the model exists locally</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>download_model_if_not_exist</span>(model_name):
</span></span><span style=display:flex><span>    model_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(<span style=color:#e6db74>&#34;models&#34;</span>, model_name)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(model_dir):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Downloading model </span><span style=color:#e6db74>{</span>model_name<span style=color:#e6db74>}</span><span style=color:#e6db74>...&#34;</span>)
</span></span><span style=display:flex><span>        tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>        model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>        tokenizer<span style=color:#f92672>.</span>save_pretrained(model_dir)
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>save_pretrained(model_dir)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Model </span><span style=color:#e6db74>{</span>model_name<span style=color:#e6db74>}</span><span style=color:#e6db74> downloaded and saved locally.&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Model </span><span style=color:#e6db74>{</span>model_name<span style=color:#e6db74>}</span><span style=color:#e6db74> already exists locally.&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model_dir
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Call the function to download the model if it doesn&#39;t exist</span>
</span></span><span style=display:flex><span>model_dir <span style=color:#f92672>=</span> download_model_if_not_exist(model_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the model and tokenizer from the local directory</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_dir)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(model_dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set the padding token and padding side</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>eos_token
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>padding_side <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;right&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Resize the model embeddings to account for the new special tokens</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>resize_token_embeddings(len(tokenizer))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set the padding token ID for generation configuration</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>generation_config<span style=color:#f92672>.</span>pad_token_id <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>pad_token_id
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Move the model to the appropriate device</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Disable caching mechanism to reduce memory usage during fine-tuning</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>use_cache <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Configure the model&#39;s pre-training tensor parallelism degree</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>pretraining_tp <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Model and tokenizer are ready for use.&#34;</span>)
</span></span></code></pre></div><h3 id=step-8-testing-the-model>Step 8: Testing the Model</h3><p>Before starting the fine-tuning process, let&rsquo;s evaluate the LLaMa3 model on a sample input to observe its initial performance. We&rsquo;ll generate responses for a few questions from the <code>test_inputs</code> list belo🌿</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_response</span>(model, prompt):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Generate a response from the model given a prompt.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model: The language model to use for generating the response.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        prompt (str): The input prompt to generate a response for.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        str: The generated response as a string.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Tokenize the input prompt and move it to the specified device</span>
</span></span><span style=display:flex><span>    input_ids <span style=color:#f92672>=</span> tokenizer(prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Generate a response from the model</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(input_ids, max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>, 
</span></span><span style=display:flex><span>                             eos_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Decode the generated tokens and return the response as a string</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_model</span>(model, test_inputs):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Quickly test the model using a set of test queries.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model: The language model to test.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        test_inputs (list of str): A list of input prompts to test the model with.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Iterate over each test input</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> input_text <span style=color:#f92672>in</span> test_inputs:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;__&#34;</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Generate a response for the input prompt</span>
</span></span><span style=display:flex><span>        generated_response <span style=color:#f92672>=</span> generate_response(model, input_text)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Print the input prompt and the generated response</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Input: </span><span style=color:#e6db74>{</span>input_text<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Generated Answer: </span><span style=color:#e6db74>{</span>generated_response<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;__&#34;</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define a list of test input prompts to evaluate the model</span>
</span></span><span style=display:flex><span>test_inputs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;How do I check the status of the RAID array on my DGX system?&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Can you show me how to get detailed information about the RAID configuration on my DGX?&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;How can I allow a user to access Docker on the DGX?&#34;</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print a message indicating the start of model testing</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Testing the model before fine-tuning:&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Test the model with the defined test inputs</span>
</span></span><span style=display:flex><span>test_model(model, test_inputs)
</span></span></code></pre></div><h3 id=step-9-load-and-inspect-the-dataset->Step 9: Load and Inspect the Dataset 📊</h3><ol><li><p><strong>Load the Dataset</strong>:</p><ul><li>Use the <code>load_dataset</code> function from the Hugging Face datasets library to load the dataset identified by <code>RayBernard/nvidia-dgx-best-practices</code>.</li><li>The dataset format is based on the Alpaca clean dataset, consisting of a JSON file with keys: <code>instruction</code>, <code>input</code>, and <code>output</code>.</li><li><a href=https://huggingface.co/datasets/RayBernard/nvidia-dgx-best-practices>Dataset Link</a></li></ul></li><li><p><strong>Split the Data</strong>:</p><ul><li>Split the dataset into training and validation sets. A good rule of thumb is to use about 20% of the data for validation.</li></ul></li><li><p><strong>Inspect the Data</strong>:</p><ul><li>Print the first instruction and response from the dataset to verify the content.</li><li>Print the total number of examples in the dataset to understand its size.</li><li>List the fields (keys) present in the dataset to understand its structure.</li><li>Print the entire dataset to get an overview of its structure and contents.</li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load a specific dataset from the Hugging Face datasets library.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;RayBernard/nvidia-dgx-best-practices&#39; is the identifier for the dataset,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># and &#39;split=&#34;train&#34;&#39; specifies that we want the training split of the dataset.</span>
</span></span><span style=display:flex><span>dataset_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;RayBernard/nvidia-dgx-best-practices&#34;</span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(dataset_name, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the first instruction and response from the dataset to verify the content.</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Instruction is: </span><span style=color:#e6db74>{</span>dataset[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;instruction&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Response is: </span><span style=color:#e6db74>{</span>dataset[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;output&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the number of examples in the dataset.</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of examples in the dataset: </span><span style=color:#e6db74>{</span>len(dataset)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the fields (keys) present in the dataset.</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Fields in the dataset: </span><span style=color:#e6db74>{</span>list(dataset<span style=color:#f92672>.</span>features<span style=color:#f92672>.</span>keys())<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the entire dataset to get an overview of its structure and contents.</span>
</span></span><span style=display:flex><span>print(dataset)
</span></span></code></pre></div><h3 id=step-10-format-and-split-the-dataset-for-training>Step 10: Format and Split the Dataset for Training</h3><p>This step ensures your dataset is properly formatted and split for the training process, making it ready for fine-tuning.</p><ol><li><p><strong>Load and Define</strong>:</p><ul><li>Load the dataset with the specified name and split. Here, we are loading the &ldquo;train&rdquo; split of the dataset.</li><li>Define the system message to be used for formatting prompts.</li></ul></li><li><p><strong>Format Prompts</strong>:</p><ul><li>Use the <code>format_prompts</code> function to format the dataset prompts according to the Meta Llama 3 Instruct prompt template with special tokens.</li><li>This function iterates over the &lsquo;instruction&rsquo; and &lsquo;output&rsquo; fields in the batch and formats them accordingly.</li><li>Apply the <code>format_prompts</code> function to the dataset in a batched manner for efficiency.</li></ul></li><li><p><strong>Split the Dataset</strong>:</p><ul><li>Split the formatted dataset into training and validation sets, using 20% of the data for validation and setting a seed for reproducibility.</li></ul></li><li><p><strong>Verify the Split</strong>:</p><ul><li>Print the number of examples in both the training and validation sets to verify the split.</li></ul></li><li><p><strong>Show Formatted Prompt</strong>:</p><ul><li>Define and use a function to show the formatted prompt for the first record, demonstrating what the prompt looks like with the system message included.</li></ul></li></ol><p>This process ensures that your dataset is well-organized and ready for the training phase, enhancing the model&rsquo;s performance during fine-tuning.`tuning phase.e.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Load the dataset with the specified name and split</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Here, we are loading the &#34;train&#34; split of the dataset</span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(dataset_name, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the system message separately</span>
</span></span><span style=display:flex><span>system_message <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;You are a helpful  linux configuration  AI, who only responds with commands used to execuite over SSH. you are to think step by step on what they are, since your job depends on it.  Format the to be place in an  ssh session&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>format_prompts</span>(batch, system_msg):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Format the prompts according to the Meta Llama 3 Instruct prompt template with special tokens.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        batch (dict): A batch of data containing &#39;instruction&#39; and &#39;output&#39; fields.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        system_msg (str): The system message to be included in the prompt.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        dict: A dictionary containing the formatted prompts under the &#39;text&#39; key.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initialize an empty list to store the formatted prompts</span>
</span></span><span style=display:flex><span>    formatted_prompts <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Iterate over the &#39;instruction&#39; and &#39;output&#39; fields in the batch</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> instruction, output <span style=color:#f92672>in</span> zip(batch[<span style=color:#e6db74>&#34;instruction&#34;</span>], batch[<span style=color:#e6db74>&#34;output&#34;</span>]):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Format the prompt according to the Meta Llama 3 Instruct template with special tokens</span>
</span></span><span style=display:flex><span>        prompt <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;&lt;|startoftext|&gt;system</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>system_msg<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;user</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>instruction<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;assistant</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>output<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># Append the formatted prompt to the list</span>
</span></span><span style=display:flex><span>        formatted_prompts<span style=color:#f92672>.</span>append(prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Return the formatted prompts as a dictionary with the key &#39;text&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;text&#34;</span>: formatted_prompts}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Apply the format_prompts function to the dataset</span>
</span></span><span style=display:flex><span><span style=color:#75715e># The function is applied in a batched manner to speed up processing</span>
</span></span><span style=display:flex><span>formatted_dataset <span style=color:#f92672>=</span> dataset<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> batch: format_prompts(batch, system_message), batched<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Split the dataset into training and validation sets</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 20% of the data is used for validation, and a seed is set for reproducibility</span>
</span></span><span style=display:flex><span>split_dataset <span style=color:#f92672>=</span> formatted_dataset<span style=color:#f92672>.</span>train_test_split(test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, seed<span style=color:#f92672>=</span><span style=color:#ae81ff>99</span>)
</span></span><span style=display:flex><span>train_dataset <span style=color:#f92672>=</span> split_dataset[<span style=color:#e6db74>&#34;train&#34;</span>]
</span></span><span style=display:flex><span>validation_dataset <span style=color:#f92672>=</span> split_dataset[<span style=color:#e6db74>&#34;test&#34;</span>]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;train dataset == &#34;</span>,train_dataset)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;validation dataset ==&#34;</span>, validation_dataset)
</span></span><span style=display:flex><span><span style=color:#75715e># Print the number of examples in the training and validation sets</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of examples in the training set: </span><span style=color:#e6db74>{</span>len(train_dataset)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of examples in the validation set: </span><span style=color:#e6db74>{</span>len(validation_dataset)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Function to show what the prompt looks like for the first record with the system message</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>show_first_prompt</span>(system_msg):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Get the first record from the dataset</span>
</span></span><span style=display:flex><span>    first_instruction <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;instruction&#34;</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    first_output <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;output&#34;</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Format the first record using the provided system message</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&lt;|startoftext|&gt;system</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>system_msg<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;user</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>first_instruction<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;assistant</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>first_output<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print the original instruction and output</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Original instruction: </span><span style=color:#e6db74>{</span>first_instruction<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Original output: </span><span style=color:#e6db74>{</span>first_output<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print the formatted prompt</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Formatted prompt with system message:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>prompt<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Show what the prompt looks like for the first record with the system message</span>
</span></span><span style=display:flex><span>show_first_prompt(system_message)
</span></span></code></pre></div><h3 id=step-11-fine-tune-the-model-and-save-the-results>Step 11: Fine-Tune the Model and Save the Results</h3><ol><li><p><strong>Setup Imports and Configurations</strong>:</p><ul><li>Import necessary libraries and modules.</li><li>Check if Intel XPU is available and set the device accordingly.</li></ul></li><li><p><strong>Load Model and Tokenizer</strong>:</p><ul><li>Load the model and tokenizer from the specified path.</li><li>Move the model&rsquo;s embedding layer to the same device and enable gradient for fine-tuning.</li></ul></li><li><p><strong>Set Environment Variables</strong>:</p><ul><li>Configure relevant environment variables for logging and configuration, including Weights and Biases project settings.</li></ul></li><li><p><strong>Load Datasets</strong>:</p><ul><li>Load the training and validation datasets.</li></ul></li><li><p><strong>Configure Training Parameters</strong>:</p><ul><li>Set training parameters including batch size, gradient accumulation steps, learning rate, and mixed precision training.</li></ul></li><li><p><strong>Initialize Trainer</strong>:</p><ul><li>Initialize the <code>SFTTrainer</code> with LoRA configuration, including training arguments and datasets.</li></ul></li><li><p><strong>Optimize Performance</strong>:</p><ul><li>Clear the XPU cache before starting the training process.</li></ul></li><li><p><strong>Begin Training</strong>:</p><ul><li>Start the training process.</li><li>Print a summary of the training results, including total training time and samples processed per second.</li><li>Handle any exceptions to ensure smooth execution.</li></ul></li><li><p><strong>Save the Model</strong>:</p><ul><li>Save the fine-tuned LoRA model to the specified path for future use.</li></ul></li></ol><p>This step-by-step approach ensures that the model is properly fine-tuned and ready for deployment, with optimal performance configurations and comprehensive logging for tracking using weights and bais site.
When you first run the training you will be prompted for an API token for wieght and bias which is free to get. Just register and account and you will get a key to input into the notebook. Lastly we can inspect our training job in real time by following the links generated by weights and bias.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> transformers
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> wandb
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM, TrainingArguments
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> trl <span style=color:#f92672>import</span> SFTTrainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> LoraConfig, get_peft_model
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> intel_extension_for_pytorch <span style=color:#66d9ef>as</span> ipex
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Torch version:&#34;</span>, torch<span style=color:#f92672>.</span>__version__)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;IPEX version:&#34;</span>, ipex<span style=color:#f92672>.</span>__version__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check if Intel XPU is available</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Intel XPU is available&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>device_count()):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU Device </span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_name(i)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Intel XPU is not available&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set the device to XPU if available, else fallback to CPU</span>
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;xpu:0&#34;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load model and tokenizer from the specified path</span>
</span></span><span style=display:flex><span>model_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Training/AI/GenAI/models/meta-llama/Meta-Llama-3-8B-Instruct&#34;</span> <span style=color:#75715e>#Model was download in Step 3</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_path)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(model_path)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>to(device)  <span style=color:#75715e># Move the model to the selected device (XPU or CPU)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Move the model&#39;s embedding layer to the same device</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>embed_tokens <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>get_input_embeddings()<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>embed_tokens<span style=color:#f92672>.</span>parameters():
</span></span><span style=display:flex><span>    param<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>  <span style=color:#75715e># Enable fine-tuning of word embeddings</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set TOKENIZERS_PARALLELISM environment variable to avoid parallelism warning</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;TOKENIZERS_PARALLELISM&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;false&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set other environment variables for logging and configuration</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;WANDB_PROJECT&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;llama3-8b-instruct-ft&#34;</span>  <span style=color:#75715e># Weights and Biases project name</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;WANDB_LOG_MODEL&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;checkpoint&#34;</span>  <span style=color:#75715e># Log model checkpoints to Weights and Biases</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;IPEX_TILE_AS_DEVICE&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;1&#34;</span>  <span style=color:#75715e># Intel Extension for PyTorch setting for optimal performance</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Configuration variables</span>
</span></span><span style=display:flex><span>finetuned_model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;RayBernard/llama3-8b-instruct-ft&#34;</span>  <span style=color:#75715e># Model ID for the fine-tuned model</span>
</span></span><span style=display:flex><span>PUSH_TO_HUB <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>  <span style=color:#75715e># Whether to push the model to the Hugging Face Hub</span>
</span></span><span style=display:flex><span>USE_WANDB <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>  <span style=color:#75715e># Whether to use Weights and Biases for logging</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load datasets (assuming these are pre-defined)</span>
</span></span><span style=display:flex><span>train_dataset <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#39;train_dataset&#39;</span>)  <span style=color:#75715e># Path to the training dataset</span>
</span></span><span style=display:flex><span>validation_dataset <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#39;validation_dataset&#39;</span>)  <span style=color:#75715e># Path to the validation dataset</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Training configuration</span>
</span></span><span style=display:flex><span>num_train_samples <span style=color:#f92672>=</span> len(train_dataset)  <span style=color:#75715e># Number of training samples</span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>  <span style=color:#75715e># Per device batch size, reduced to fit memory</span>
</span></span><span style=display:flex><span>gradient_accumulation_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>  <span style=color:#75715e># Accumulate gradients over 8 steps to simulate a larger batch size</span>
</span></span><span style=display:flex><span>steps_per_epoch <span style=color:#f92672>=</span> num_train_samples <span style=color:#f92672>//</span> (batch_size <span style=color:#f92672>*</span> gradient_accumulation_steps)  <span style=color:#75715e># Steps per epoch</span>
</span></span><span style=display:flex><span>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>  <span style=color:#75715e># Number of training epochs</span>
</span></span><span style=display:flex><span>max_steps <span style=color:#f92672>=</span> steps_per_epoch <span style=color:#f92672>*</span> num_epochs  <span style=color:#75715e># Total number of training steps</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Finetuning for max number of steps: </span><span style=color:#e6db74>{</span>max_steps<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>print_training_summary</span>(results):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Time: </span><span style=color:#e6db74>{</span>results<span style=color:#f92672>.</span>metrics[<span style=color:#e6db74>&#39;train_runtime&#39;</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74> .2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)  <span style=color:#75715e># Print total training time</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Samples/second: </span><span style=color:#e6db74>{</span>results<span style=color:#f92672>.</span>metrics[<span style=color:#e6db74>&#39;train_samples_per_second&#39;</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74> .2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)  <span style=color:#75715e># Print training speed</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>training_args <span style=color:#f92672>=</span> TrainingArguments(
</span></span><span style=display:flex><span>    per_device_train_batch_size<span style=color:#f92672>=</span>batch_size,  <span style=color:#75715e># Batch size per device (GPU or XPU)</span>
</span></span><span style=display:flex><span>    gradient_accumulation_steps<span style=color:#f92672>=</span>gradient_accumulation_steps,  <span style=color:#75715e># Gradient accumulation steps to save memory</span>
</span></span><span style=display:flex><span>    warmup_ratio<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>,  <span style=color:#75715e># Ratio of total steps for learning rate warmup to stabilize training</span>
</span></span><span style=display:flex><span>    max_steps<span style=color:#f92672>=</span>max_steps,  <span style=color:#75715e># Total number of training steps calculated from epochs and batch size</span>
</span></span><span style=display:flex><span>    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>3e-5</span>,  <span style=color:#75715e># Learning rate for the optimizer</span>
</span></span><span style=display:flex><span>    evaluation_strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;steps&#34;</span>,  <span style=color:#75715e># Evaluation strategy to evaluate the model at regular steps</span>
</span></span><span style=display:flex><span>    save_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,  <span style=color:#75715e># Frequency (in steps) to save model checkpoints</span>
</span></span><span style=display:flex><span>    fp16<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,  <span style=color:#75715e># Enable mixed precision training (16-bit floating point numbers) to save memory</span>
</span></span><span style=display:flex><span>    logging_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,  <span style=color:#75715e># Frequency (in steps) to log training metrics</span>
</span></span><span style=display:flex><span>    output_dir<span style=color:#f92672>=</span>finetuned_model_id,  <span style=color:#75715e># Directory to save the model and training outputs</span>
</span></span><span style=display:flex><span>    hub_model_id<span style=color:#f92672>=</span>finetuned_model_id <span style=color:#66d9ef>if</span> PUSH_TO_HUB <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>,  <span style=color:#75715e># Model ID for pushing to Hugging Face Hub</span>
</span></span><span style=display:flex><span>    report_to<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;wandb&#34;</span> <span style=color:#66d9ef>if</span> USE_WANDB <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>,  <span style=color:#75715e># Reporting to Weights and Biases for experiment tracking</span>
</span></span><span style=display:flex><span>    push_to_hub<span style=color:#f92672>=</span>PUSH_TO_HUB,  <span style=color:#75715e># Whether to push the model to the Hugging Face Hub</span>
</span></span><span style=display:flex><span>    max_grad_norm<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>,  <span style=color:#75715e># Max gradient norm for gradient clipping to prevent exploding gradients</span>
</span></span><span style=display:flex><span>    weight_decay<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>,  <span style=color:#75715e># Weight decay for regularization to prevent overfitting</span>
</span></span><span style=display:flex><span>    group_by_length<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,  <span style=color:#75715e># Group sequences by length to improve training efficiency</span>
</span></span><span style=display:flex><span>    gradient_checkpointing<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>  <span style=color:#75715e># Enable gradient checkpointing to save memory by trading compute</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize the SFTTrainer with LoRA configuration</span>
</span></span><span style=display:flex><span>lora_config <span style=color:#f92672>=</span> LoraConfig()  <span style=color:#75715e># LoRA configuration (assumed to be defined)</span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model,  <span style=color:#75715e># Model to train</span>
</span></span><span style=display:flex><span>    train_dataset<span style=color:#f92672>=</span>train_dataset,  <span style=color:#75715e># Training dataset</span>
</span></span><span style=display:flex><span>    eval_dataset<span style=color:#f92672>=</span>validation_dataset,  <span style=color:#75715e># Validation dataset</span>
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>=</span>tokenizer,  <span style=color:#75715e># Tokenizer</span>
</span></span><span style=display:flex><span>    args<span style=color:#f92672>=</span>training_args,  <span style=color:#75715e># Training arguments</span>
</span></span><span style=display:flex><span>    peft_config<span style=color:#f92672>=</span>lora_config,  <span style=color:#75715e># LoRA configuration</span>
</span></span><span style=display:flex><span>    dataset_text_field<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;text&#34;</span>,  <span style=color:#75715e># Field name in the dataset containing the text data</span>
</span></span><span style=display:flex><span>    max_seq_length<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>,  <span style=color:#75715e># Maximum sequence length for training</span>
</span></span><span style=display:flex><span>    packing<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>  <span style=color:#75715e># Enable sequence packing for efficiency</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># Clear XPU cache before starting the training</span>
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>empty_cache()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Start training</span>
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> trainer<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print training summary</span>
</span></span><span style=display:flex><span>    print_training_summary(results)
</span></span><span style=display:flex><span>    wandb<span style=color:#f92672>.</span>finish()  <span style=color:#75715e># Finish the wandb run</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error during training: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)  <span style=color:#75715e># Print any errors that occur during training</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save the fine-tuned LoRA model</span>
</span></span><span style=display:flex><span>tuned_lora_model <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;llama3-8b-instruct-ftuned&#34;</span>
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>save_pretrained(tuned_lora_model)  <span style=color:#75715e># Save the trained model to the specified path</span>
</span></span></code></pre></div><h3 id=step-12-merge-and-save-the-fine-tuned-model>Step 12: Merge and Save the Fine-Tuned Model</h3><p>After fine-tuning the model, merge the fine-tuned LoRA model with the base model and save the final tuned model. This process ensures that the fine-tuning adjustments are integrated into the base model, resulting in an optimized and ready-to-use model.</p><ol><li><strong>Import Required Libraries</strong>: Import the necessary libraries from <code>peft</code> and <code>transformers</code>.</li><li><strong>Load Base Model</strong>: Load the base model using <code>AutoModelForCausalLM</code> with the specified model ID and configurations to optimize memory usage and performance.</li><li><strong>Merge Models</strong>: Use <code>PeftModel</code> to load the fine-tuned LoRA model and merge it with the base model.</li><li><strong>Unload Unnecessary Parameters</strong>: Merge and unload unnecessary parameters from the model to optimize it.</li><li><strong>Save the Final Model</strong>: Save the final merged model to the specified path for future use.</li></ol><p>This step finalizes the training process by producing a single, fine-tuned model ready for del.save_pretrained(tunmodel)</p><pre tabindex=0><code>

```python
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments

tuned_model = &#34;RayBernard/llama3-8b-instruct-ft-dgx&#34;

base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.bfloat16,
)

model = PeftModel.from_pretrained(base_model, tuned_lora_model)
model = model.merge_and_unload()
# save final tuned model
model.save_pretrained(tuned_model)
</code></pre><h3 id=step-13-upload-the-fine-tuned-model-to-hugging-face-hub->Step 13: Upload the Fine-Tuned Model to Hugging Face Hub 🚀</h3><ol><li><p><strong>Install Necessary Libraries</strong>:</p><ul><li>Ensure you have the <code>huggingface_hub</code> library installed.</li></ul></li><li><p><strong>Import Libraries</strong>:</p><ul><li>Import the necessary modules for interacting with the Hugging Face Hub.</li></ul></li><li><p><strong>Authenticate with Hugging Face Hub</strong>:</p><ul><li>Set your Hugging Face token as an environment variable.</li><li>Log in to Hugging Face Hub using the token.</li></ul></li><li><p><strong>Define the Path and Repository</strong>:</p><ul><li>Specify the path to your fine-tuned model.</li><li>Define the name of the repository you want to create on Hugging Face Hub.</li></ul></li><li><p><strong>Upload Files to Hugging Face Hub</strong>:</p><ul><li>Use the <code>HfApi</code> class to create the repository if it doesn&rsquo;t already exist.</li><li>Upload all files from the specified path to the repository.</li></ul></li></ol><p>This step ensures your fine-tuned model is uploaded to the Hugging Face Hub, making it accessible for future use and sharing with the community. This process uploads your fine-tuned model to the Hugging Face Hub, making it available for easy access and sharing.ss and sharing.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Step 1: Install Necessary Libraries</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>pip install huggingface_hub
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 2: Import Libraries</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> huggingface_hub <span style=color:#f92672>import</span> HfApi, login
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 3: Authenticate with Hugging Face Hub</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Make sure to set your Hugging Face token in the environment variable</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;HUGGINGFACE_TOKEN&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;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&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Login to Hugging Face Hub</span>
</span></span><span style=display:flex><span>login(token<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#39;HUGGINGFACE_TOKEN&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 4: Define the Path and Repository</span>
</span></span><span style=display:flex><span>model_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;RayBernard/llama3-8b-instruct-ft-dgx&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Name of the repo you want to create on huggingface </span>
</span></span><span style=display:flex><span>repository_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;RayBernard/llama3-8b-instruct-ft-dgx&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 5: Upload Files to Hugging Face Hub</span>
</span></span><span style=display:flex><span>api <span style=color:#f92672>=</span> HfApi()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create the repository if it doesn&#39;t exist</span>
</span></span><span style=display:flex><span>api<span style=color:#f92672>.</span>create_repo(repo_id<span style=color:#f92672>=</span>repository_name, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Upload all files from the specified path to the repository</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> root, _, files <span style=color:#f92672>in</span> os<span style=color:#f92672>.</span>walk(model_path):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> file <span style=color:#f92672>in</span> files:
</span></span><span style=display:flex><span>        file_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root, file)
</span></span><span style=display:flex><span>        api<span style=color:#f92672>.</span>upload_file(
</span></span><span style=display:flex><span>            path_or_fileobj<span style=color:#f92672>=</span>file_path,
</span></span><span style=display:flex><span>            path_in_repo<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>relpath(file_path, model_path),
</span></span><span style=display:flex><span>            repo_id<span style=color:#f92672>=</span>repository_name
</span></span><span style=display:flex><span>        )
</span></span></code></pre></div><h3 id=step-14-fine-tuning-results-and-observations>Step 14: Fine-Tuning Results and Observations</h3><p>After fine-tuning the LLaMA3 model oourse question-answering dataset, we observed significant improvements in the model&rsquo;s ability to provide accurate and relevant responses to a wide range of queries. The fine-tuned model demonstrated a better understanding of domain-specific terminology and concepts compared to the baseline model.</p><p>The model&rsquo;s performance was evaluated on a held-out test set, achieving promising results in terms of accuracy and coherence. The fine-tuned model was able to generate more contextually appropriate and informative responses compared to the generic model.</p><p>However, it is important to note that the model&rsquo;s performance may still be limited by the size and diversity of the fine-tuning dataset. Expanding the dataset with more varied questions and answers across different domains could further enhance the model&rsquo;s capabilities and generalization.</p><p>Overall, the fine-tuned model shows promise in assisting users with their information needs across various topics, but it should be used as a complementary tool alongside other reliable sources of information.rmation.rmation.rmation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> PeftModel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the local path to the model</span>
</span></span><span style=display:flex><span>local_model_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;llama3-8b-instruct-ft-dgx&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the tokenizer</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(local_model_path)
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>eos_token
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>padding_side <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;right&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(local_model_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the PEFT fine-tuned model, if applicable</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> PeftModel<span style=color:#f92672>.</span>from_pretrained(model, local_model_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Move the model to the correct device</span>
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;xpu:0&#34;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Test inputs</span>
</span></span><span style=display:flex><span>test_inputs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#34;How do I check the status of the RAID array on my DGX system?&#34;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#34;Can you show me how to get detailed information about the RAID configuration on my DGX?&#34;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#34;How can I allow a user to access Docker on the DGX?&#34;</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run inference on test inputs</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> text <span style=color:#f92672>in</span> test_inputs:
</span></span><span style=display:flex><span>    <span style=color:#75715e># Tokenize the input text and convert to PyTorch tensors, then move to the selected device (XPU or CPU)</span>
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tokenizer(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Generate text based on the input, with the following parameters:</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>        <span style=color:#f92672>**</span>inputs,  <span style=color:#75715e># Pass the tokenized inputs to the model</span>
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,  <span style=color:#75715e># Maximum number of new tokens to generate</span>
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,  <span style=color:#75715e># Use sampling for generation (as opposed to greedy decoding)</span>
</span></span><span style=display:flex><span>        top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,  <span style=color:#75715e># Use top-k sampling, considering the top 100 tokens</span>
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,  <span style=color:#75715e># Sampling temperature; higher values mean more randomness</span>
</span></span><span style=display:flex><span>        eos_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id  <span style=color:#75715e># End-of-sequence token ID to stop generation</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Decode the generated tokens back to text and print the result, skipping special tokens</span>
</span></span><span style=display:flex><span>    print(tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>))
</span></span></code></pre></div><h3 id=happy-fine-tuning->Happy Fine-Tuning! 😄✨</h3><p>Congratulations on reaching this exciting milestone! You now possess the tools and knowledge to fine-tune the powerful LLaMA 3 model on your own custom datasets. This achievement opens up a world of possibilities for you to explore and unleash the full potential of this cutting-edge language model.
We encourage you to embrace the spirit of experimentation and exploration. Feel free to customize and adapt this notebook to fit your specific use case. Try different datasets, tweak the hyperparameters, and observe how the model&rsquo;s performance evolves. This hands-on experience will deepen your understanding and allow you to tailor the model to your unique requirements.
Moreover, we invite you to share your fine-tuned models and experiences with the broader community. Consider open-sourcing your work on platforms like GitHub or Hugging Face, and write blog posts to detail your fine-tuning journey. Your insights and achievements can inspire and assist others who are embarking on their own fine-tuning projects, fostering a collaborative and supportive environment for knowledge sharing.
If you encounter any challenges or have suggestions for improvement, please don&rsquo;t hesitate to reach out and provide feedback. We value your input and are committed to making this notebook and the fine-tuning process as smooth and enjoyable as possible. Your feedback will help us refine and enhance the resources available to the community.
Remember, the journey of fine-tuning language models is an iterative and continuous process. Embrace the challenges, celebrate your successes, and continue pushing the boundaries of what&rsquo;s possible. Together, we can unlock the full potential of these powerful models and drive innovation in various domains.</p><p>SPDX-License-Identifier: Apache-2.0
Copyright (c) 2024, Rahul Unnikrishnan Nair <a href=mailto:rahul.unnikrishnan.nair@intel.com>rahul.unnikrishnan.nair@intel.com</a>
Ray Bernard <a href=mailto:ray.bernard@outlook.com>ray.bernard@outlook.com</a></p><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://raymondbernard-github-io.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><script src=https://formspree.io/js/formbutton-v1.min.js defer></script><script>window.formbutton=window.formbutton||function(){(formbutton.q=formbutton.q||[]).push(arguments)},formbutton("create",{action:"https://formspree.io/f/xkndlbok",title:"How can we help?",fields:[{type:"email",label:"Email:",name:"email",required:!0,placeholder:"your@email.com"},{type:"textarea",label:"Message:",name:"message",placeholder:"What's on your mind?"},{type:"submit"}],styles:{title:{backgroundColor:"gray"},button:{backgroundColor:"gray"}}})</script><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://raymondbernard.github.io/>&copy; Ray Bernard's Fine-Tune Journal 2024</a><div><div class=ananke-socials><a href=https://www.github.com/raymondbernard/fine-tune-journal target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="github link" aria-label="follow on github——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.linkedin.com/in/raymond-bernard-960382/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="My Linkedin link" aria-label="follow on My Linkedin——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q target=_blank rel=noopener class="youtube ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="youtube link" aria-label="follow on youtube——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527 41.34c-.278.0-.478.078-.6.244-.121.156-.18.424-.18.796v.896h1.543V42.38c0-.372-.062-.64-.185-.796C42.989 41.418 42.792 41.34 42.527 41.34zM36.509 41.309c.234.0.417.076.544.23.123.155.185.383.185.682v4.584c0 .286-.053.487-.153.611-.1.127-.256.189-.47.189-.148.0-.287-.033-.421-.096-.135-.062-.274-.171-.415-.313v-5.531c.119-.122.239-.213.36-.271C36.26 41.335 36.383 41.309 36.509 41.309zm5.239 3.349v1.672c0 .468.057.792.17.974.118.181.313.269.592.269.289.0.491-.076.606-.229.114-.153.175-.489.175-1.013v-.405h1.795v.456c0 .911-.217 1.596-.657 2.059-.435.459-1.089.687-1.958.687-.781.0-1.398-.242-1.847-.731-.448-.486-.676-1.157-.676-2.014v-3.986c0-.768.249-1.398.742-1.882.493-.484 1.128-.727 1.911-.727.799.0 1.413.225 1.843.674.429.448.642 1.093.642 1.935v2.264H41.748zm-3.125 3.837c-.271.336-.669.501-1.187.501-.343.0-.646-.062-.912-.192-.267-.129-.519-.327-.746-.601v.681h-1.764V36.852h1.764v3.875c.237-.27.485-.478.748-.616.267-.143.534-.212.805-.212.554.0.975.189 1.265.565.294.379.438.933.438 1.66v4.926C39.034 47.678 38.897 48.159 38.623 48.495zM30.958 48.884v-.976c-.325.361-.658.636-1.009.822-.349.191-.686.282-1.014.282-.405.0-.705-.129-.913-.396-.201-.266-.305-.658-.305-1.189v-7.422h1.744v6.809c0 .211.037.362.107.457.077.095.196.141.358.141.128.0.292-.062.488-.188.197-.125.375-.283.542-.475v-6.744H32.7v8.878H30.958zM24.916 38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916zm8.078-5.622c0-.001 12.08.018 13.514 1.45 1.439 1.435 1.455 8.514 1.455 8.555.0.0-.012 7.117-1.455 8.556C45.074 52.969 32.994 53 32.994 53s-12.079-.031-13.516-1.462c-1.438-1.435-1.441-8.502-1.441-8.556.0-.041.004-7.12 1.441-8.555 1.438-1.431 13.516-1.45 13.516-1.449zm9.526-3.723h-1.966v-1.08c-.358.397-.736.703-1.13.909-.392.208-.771.312-1.14.312-.458.0-.797-.146-1.027-.437-.229-.291-.345-.727-.345-1.311v-8.172h1.962v7.497c0 .231.045.399.127.502.08.104.216.156.399.156.143.0.327-.069.548-.206.22-.137.423-.312.605-.527v-7.422h1.966V29.255zM31.847 27.588c.139.147.339.219.6.219.266.0.476-.075.634-.223.157-.152.235-.358.235-.618v-5.327c0-.214-.08-.387-.241-.519-.16-.131-.37-.196-.628-.196-.241.0-.435.065-.586.196-.148.132-.225.305-.225.519v5.327C31.636 27.233 31.708 27.439 31.847 27.588zm-1.439-7.685c.528-.449 1.241-.674 2.132-.674.812.0 1.48.237 2.001.711.517.473.777 1.083.777 1.828v5.051c0 .836-.255 1.491-.762 1.968-.513.476-1.212.714-2.106.714-.858.0-1.547-.246-2.064-.736-.513-.492-.772-1.152-.772-1.983v-5.068C29.613 20.954 29.877 20.351 30.408 19.903zM24.262 16h-2.229l2.634 8.003v5.252h2.213v-5.5L29.454 16h-2.25l-1.366 5.298h-.139L24.262 16zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://x.com/raybernard007 target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://medium.com/@raybernard007 target=_blank rel=noopener class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 170 170" viewBox="0 0 170 170" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M46.5340803 65.2157554C46.6968378 63.6076572 46.0836 62.018231 44.8828198 60.93592L32.6512605 46.2010582V44H70.6302521l29.3557423 64.380952L125.794585 44H162v2.2010582L151.542017 56.2281011C150.640424 56.9153477 150.193188 58.0448862 150.380019 59.1628454V132.837155C150.193188 133.955114 150.640424 135.084652 151.542017 135.771899l10.213352 10.027043V148H110.38282V145.798942l10.580299-10.271605c1.039682-1.039389 1.039682-1.345091 1.039682-2.934744V73.0417402l-29.4169 74.7136978H88.6106443L54.3622782 73.0417402V123.115814C54.0767278 125.221069 54.7759199 127.3406 56.2581699 128.863022L70.0186741 145.55438V147.755438H31V145.55438l13.7605042-16.691358c1.4714579-1.524946 2.1298796-3.658537 1.7735761-5.747208V65.2157554z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.reddit.com/user/OpenAITutor/ target=_blank rel=noopener class="reddit ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="reddit link" aria-label="follow on reddit——Opens in a new window">reddit
<span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>