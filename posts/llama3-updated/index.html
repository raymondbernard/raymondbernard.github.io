<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Instruct on Intel Max Series GPUs | Ray Bernard's Fine Tuned Journal</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.
I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace."><meta name=generator content="Hugo 0.126.2"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link rel=canonical href=http://localhost:1313/posts/llama3-updated/><meta property="og:url" content="http://localhost:1313/posts/llama3-updated/"><meta property="og:site_name" content="Ray Bernard's Fine Tuned Journal"><meta property="og:title" content="Instruct on Intel Max Series GPUs"><meta property="og:description" content="Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.
I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-14T16:05:44-04:00"><meta property="article:modified_time" content="2024-06-14T16:05:44-04:00"><meta itemprop=name content="Instruct on Intel Max Series GPUs"><meta itemprop=description content="Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.
I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace."><meta itemprop=datePublished content="2024-06-14T16:05:44-04:00"><meta itemprop=dateModified content="2024-06-14T16:05:44-04:00"><meta itemprop=wordCount content="5197"><meta name=twitter:card content="summary"><meta name=twitter:title content="Instruct on Intel Max Series GPUs"><meta name=twitter:description content="Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.
I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace."><script>(function(e,t,n,s,o,i){e.hj=e.hj||function(){(e.hj.q=e.hj.q||[]).push(arguments)},e._hjSettings={hjid:5009664,hjsv:6},o=t.getElementsByTagName("head")[0],i=t.createElement("script"),i.async=1,i.src=n+e._hjSettings.hjid+s+e._hjSettings.hjsv,o.appendChild(i)})(window,document,"https://static.hotjar.com/c/hotjar-",".js?sv=")</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-XT709SJYMZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XT709SJYMZ")</script></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Ray Bernard's Fine Tuned Journal</a><div class="flex-l items-center"><div class=ananke-socials><a href=https://www.github.com/raymondbernard/fine-tune-journal target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="github link" aria-label="follow on github——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.linkedin.com/in/raymond-bernard-960382/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="My Linkedin link" aria-label="follow on My Linkedin——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q target=_blank rel=noopener class="youtube ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="youtube link" aria-label="follow on youtube——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527 41.34c-.278.0-.478.078-.6.244-.121.156-.18.424-.18.796v.896h1.543V42.38c0-.372-.062-.64-.185-.796C42.989 41.418 42.792 41.34 42.527 41.34zM36.509 41.309c.234.0.417.076.544.23.123.155.185.383.185.682v4.584c0 .286-.053.487-.153.611-.1.127-.256.189-.47.189-.148.0-.287-.033-.421-.096-.135-.062-.274-.171-.415-.313v-5.531c.119-.122.239-.213.36-.271C36.26 41.335 36.383 41.309 36.509 41.309zm5.239 3.349v1.672c0 .468.057.792.17.974.118.181.313.269.592.269.289.0.491-.076.606-.229.114-.153.175-.489.175-1.013v-.405h1.795v.456c0 .911-.217 1.596-.657 2.059-.435.459-1.089.687-1.958.687-.781.0-1.398-.242-1.847-.731-.448-.486-.676-1.157-.676-2.014v-3.986c0-.768.249-1.398.742-1.882.493-.484 1.128-.727 1.911-.727.799.0 1.413.225 1.843.674.429.448.642 1.093.642 1.935v2.264H41.748zm-3.125 3.837c-.271.336-.669.501-1.187.501-.343.0-.646-.062-.912-.192-.267-.129-.519-.327-.746-.601v.681h-1.764V36.852h1.764v3.875c.237-.27.485-.478.748-.616.267-.143.534-.212.805-.212.554.0.975.189 1.265.565.294.379.438.933.438 1.66v4.926C39.034 47.678 38.897 48.159 38.623 48.495zM30.958 48.884v-.976c-.325.361-.658.636-1.009.822-.349.191-.686.282-1.014.282-.405.0-.705-.129-.913-.396-.201-.266-.305-.658-.305-1.189v-7.422h1.744v6.809c0 .211.037.362.107.457.077.095.196.141.358.141.128.0.292-.062.488-.188.197-.125.375-.283.542-.475v-6.744H32.7v8.878H30.958zM24.916 38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916zm8.078-5.622c0-.001 12.08.018 13.514 1.45 1.439 1.435 1.455 8.514 1.455 8.555.0.0-.012 7.117-1.455 8.556C45.074 52.969 32.994 53 32.994 53s-12.079-.031-13.516-1.462c-1.438-1.435-1.441-8.502-1.441-8.556.0-.041.004-7.12 1.441-8.555 1.438-1.431 13.516-1.45 13.516-1.449zm9.526-3.723h-1.966v-1.08c-.358.397-.736.703-1.13.909-.392.208-.771.312-1.14.312-.458.0-.797-.146-1.027-.437-.229-.291-.345-.727-.345-1.311v-8.172h1.962v7.497c0 .231.045.399.127.502.08.104.216.156.399.156.143.0.327-.069.548-.206.22-.137.423-.312.605-.527v-7.422h1.966V29.255zM31.847 27.588c.139.147.339.219.6.219.266.0.476-.075.634-.223.157-.152.235-.358.235-.618v-5.327c0-.214-.08-.387-.241-.519-.16-.131-.37-.196-.628-.196-.241.0-.435.065-.586.196-.148.132-.225.305-.225.519v5.327C31.636 27.233 31.708 27.439 31.847 27.588zm-1.439-7.685c.528-.449 1.241-.674 2.132-.674.812.0 1.48.237 2.001.711.517.473.777 1.083.777 1.828v5.051c0 .836-.255 1.491-.762 1.968-.513.476-1.212.714-2.106.714-.858.0-1.547-.246-2.064-.736-.513-.492-.772-1.152-.772-1.983v-5.068C29.613 20.954 29.877 20.351 30.408 19.903zM24.262 16h-2.229l2.634 8.003v5.252h2.213v-5.5L29.454 16h-2.25l-1.366 5.298h-.139L24.262 16zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://x.com/raybernard007 target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://medium.com/@raybernard007 target=_blank rel=noopener class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 170 170" viewBox="0 0 170 170" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M46.5340803 65.2157554C46.6968378 63.6076572 46.0836 62.018231 44.8828198 60.93592L32.6512605 46.2010582V44H70.6302521l29.3557423 64.380952L125.794585 44H162v2.2010582L151.542017 56.2281011C150.640424 56.9153477 150.193188 58.0448862 150.380019 59.1628454V132.837155C150.193188 133.955114 150.640424 135.084652 151.542017 135.771899l10.213352 10.027043V148H110.38282V145.798942l10.580299-10.271605c1.039682-1.039389 1.039682-1.345091 1.039682-2.934744V73.0417402l-29.4169 74.7136978H88.6106443L54.3622782 73.0417402V123.115814C54.0767278 125.221069 54.7759199 127.3406 56.2581699 128.863022L70.0186741 145.55438V147.755438H31V145.55438l13.7605042-16.691358c1.4714579-1.524946 2.1298796-3.658537 1.7735761-5.747208V65.2157554z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.reddit.com/user/OpenAITutor/ target=_blank rel=noopener class="reddit ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="reddit link" aria-label="follow on reddit——Opens in a new window">reddit
<span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/posts/llama3-updated/&amp;title=Instruct%20on%20Intel%20Max%20Series%20GPUs" class="ananke-social-link linkedin no-underline" aria-label="share on My Linkedin"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href="https://twitter.com/intent/tweet?url=http://localhost:1313/posts/llama3-updated/&amp;text=Instruct%20on%20Intel%20Max%20Series%20GPUs" class="ananke-social-link twitter no-underline" aria-label="share on twitter"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div><h1 class="f1 athelas mt3 mb1">Instruct on Intel Max Series GPUs</h1><time class="f6 mv4 dib tracked" datetime=2024-06-14T16:05:44-04:00>June 14, 2024</time>
<span class="f6 mv4 dib tracked">- 25 minutes read </span><span class="f6 mv4 dib tracked">- 5197 words</span></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><img src=/images/llama3.png alt="alt text"></p><h2 id=fine-tuning-llama-3-8b-instruct-on-intel-max-series-gpus-an-exciting-journey>Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey</h2><p>In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.</p><p>I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace. By importing my dataset, obtaining the LLaMa 3 8B Instruct model, and training it using the Transformer Trainer, I was able to achieve outstanding results.</p><p>The entire training process was meticulously monitored by Weights and Biases, providing detailed insights into memory usage, disk I/O, training loss, and more. It&rsquo;s truly an outstanding product!</p><p>The best part? It&rsquo;s all free! I was amazed by the capabilities of the Intel Developer Cloud, particularly for machine learning (ML), high-performance computing (HPC), and generative AI (GenAI) workflows. The Intel® Data Center Max 1100 GPU, a high-performance 300-watt double-wide AIC card, features 56 Xe cores and 48 GB of HBM2E memory, delivering exceptional performance.</p><p>In the spirit of open source, I developed the following notebook, partially based on the original work of Rahul Unnikrishnan Nair from Intel, &ldquo;Fine-tuning Google&rsquo;s Gemma Model on Intel Max Series GPUs.&rdquo; Thank you for the foundation. I was able to significantly tweak and create a process to fine-tune LLaMa 3 models, which represent a significant leap from LLaMa 2 in terms of capabilities. It&rsquo;s amazing how such small models can produce great results with quality data.</p><p>I will be producing a detailed video review of the notebook, which you can find on my YouTube channel: <a href=https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q>YouTube Channel</a>.</p><p><strong>Note: This code was executed in a Jupyter notebook on Intel&rsquo;s Developer Cloud.</strong></p><h3 id=set-path-if-you-receive-a-path-error-while-installing-software>Set Path if you receive a path error while installing software</h3><p>The below sets the path for your .local/bin .
The root directory needs to be change to your root directory. &lsquo;/home//.local/bin&rsquo;
You can find out your userid by launching a terminal executing at the prompt $ pwd</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>user_dir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/home/u2b3e96b2fc320ef8c781f51df67225d/&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add the directory to the PATH</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;PATH&#39;</span>] <span style=color:#f92672>+=</span> os<span style=color:#f92672>.</span>pathsep <span style=color:#f92672>+</span> user_dir <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.local/bin&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Verify the PATH update</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Updated PATH:&#34;</span>, os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;PATH&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check if the directory is now in PATH</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> user_dir <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.local/bin&#39;</span> <span style=color:#f92672>in</span> os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;PATH&#39;</span>]:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Directory successfully added to PATH.&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Failed to add directory to PATH.&#34;</span>)
</span></span></code></pre></div><h3 id=run-only-once-to-make-sure-you-have-the-proper-versions-of-the-base-software-needed>Run only once to make sure you have the proper versions of the base software needed.</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>python <span style=color:#f92672>-</span>m pip install torch<span style=color:#f92672>==</span><span style=color:#ae81ff>2.1.0</span><span style=color:#f92672>.</span>post2 torchvision<span style=color:#f92672>==</span><span style=color:#ae81ff>0.16.0</span><span style=color:#f92672>.</span>post2 torchaudio<span style=color:#f92672>==</span><span style=color:#ae81ff>2.1.0</span><span style=color:#f92672>.</span>post2 intel<span style=color:#f92672>-</span>extension<span style=color:#f92672>-</span><span style=color:#66d9ef>for</span><span style=color:#f92672>-</span>pytorch<span style=color:#f92672>==</span><span style=color:#ae81ff>2.1.30</span><span style=color:#f92672>.</span>post0 oneccl_bind_pt<span style=color:#f92672>==</span><span style=color:#ae81ff>2.1.300</span><span style=color:#f92672>+</span>xpu <span style=color:#f92672>--</span>extra<span style=color:#f92672>-</span>index<span style=color:#f92672>-</span>url https:<span style=color:#f92672>//</span>pytorch<span style=color:#f92672>-</span>extension<span style=color:#f92672>.</span>intel<span style=color:#f92672>.</span>com<span style=color:#f92672>/</span>release<span style=color:#f92672>-</span>whl<span style=color:#f92672>/</span>stable<span style=color:#f92672>/</span>xpu<span style=color:#f92672>/</span>us<span style=color:#f92672>/</span>
</span></span></code></pre></div><h3 id=step-1-initial-setup>Step 1: Initial Setup</h3><p>Before we begin the fine-tuning process, it&rsquo;s essential to ensure that we have the proper libraries installed and the correct kernel configured. This step needs to be performed only once.
First, make sure you are using the Modin kernel on the Intel Developer Cloud. The Modin kernel is a specialized kernel designed for efficient data processing and analysis. To access the Modin kernel, follow these steps:
Join the Intel Developer Cloud by creating an account.
Once logged in, navigate to the &ldquo;Free Training&rdquo; section.
You will be presented with a Jupyter Lab environment, where you can select the Modin kernel.Run only once to make sure you have the proper versions of the additional software needed.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> site
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Install the required packages</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>{sys<span style=color:#f92672>.</span>executable} <span style=color:#f92672>-</span>m pip install <span style=color:#f92672>--</span>upgrade <span style=color:#e6db74>&#34;transformers&gt;=4.38.*&#34;</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>{sys<span style=color:#f92672>.</span>executable} <span style=color:#f92672>-</span>m pip install <span style=color:#f92672>--</span>upgrade <span style=color:#e6db74>&#34;datasets&gt;=2.18.*&#34;</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>{sys<span style=color:#f92672>.</span>executable} <span style=color:#f92672>-</span>m pip install <span style=color:#f92672>--</span>upgrade <span style=color:#e6db74>&#34;wandb&gt;=0.17.*&#34;</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>{sys<span style=color:#f92672>.</span>executable} <span style=color:#f92672>-</span>m pip install <span style=color:#f92672>--</span>upgrade <span style=color:#e6db74>&#34;trl&gt;=0.7.11&#34;</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>{sys<span style=color:#f92672>.</span>executable} <span style=color:#f92672>-</span>m pip install <span style=color:#f92672>--</span>upgrade <span style=color:#e6db74>&#34;peft&gt;=0.9.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>{sys<span style=color:#f92672>.</span>executable} <span style=color:#f92672>-</span>m pip install <span style=color:#f92672>--</span>upgrade <span style=color:#e6db74>&#34;accelerate&gt;=0.28.*&#34;</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>{sys<span style=color:#f92672>.</span>executable} <span style=color:#f92672>-</span>m pip install <span style=color:#f92672>--</span>upgrade <span style=color:#e6db74>&#34;huggingface_hub&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Get the site-packages directory</span>
</span></span><span style=display:flex><span>site_packages_dir <span style=color:#f92672>=</span> site<span style=color:#f92672>.</span>getsitepackages()[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add the site pkg directory where these pkgs are insalled to the top of sys.path</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>access(site_packages_dir, os<span style=color:#f92672>.</span>W_OK):
</span></span><span style=display:flex><span>    user_site_packages_dir <span style=color:#f92672>=</span> site<span style=color:#f92672>.</span>getusersitepackages()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> user_site_packages_dir <span style=color:#f92672>in</span> sys<span style=color:#f92672>.</span>path:
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>remove(user_site_packages_dir)
</span></span><span style=display:flex><span>    sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, user_site_packages_dir)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> site_packages_dir <span style=color:#f92672>in</span> sys<span style=color:#f92672>.</span>path:
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>remove(site_packages_dir)
</span></span><span style=display:flex><span>    sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, site_packages_dir)
</span></span></code></pre></div><h2 id=optionally-check-to-see-if-have-installed-versions-of-the-base-software>Optionally, Check to see if have installed versions of the base software</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Import necessary libraries</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> transformers
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> wandb
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> trl
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> peft
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> datasets
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Get versions of the libraries</span>
</span></span><span style=display:flex><span>torch_version <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>__version__
</span></span><span style=display:flex><span>transformers_version <span style=color:#f92672>=</span> transformers<span style=color:#f92672>.</span>__version__
</span></span><span style=display:flex><span>wandb_version <span style=color:#f92672>=</span> wandb<span style=color:#f92672>.</span>__version__
</span></span><span style=display:flex><span>trl_version <span style=color:#f92672>=</span> trl<span style=color:#f92672>.</span>__version__
</span></span><span style=display:flex><span>peft_version <span style=color:#f92672>=</span> peft<span style=color:#f92672>.</span>__version__
</span></span><span style=display:flex><span>datasets_version <span style=color:#f92672>=</span> datasets<span style=color:#f92672>.</span>__version__
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the versions</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;torch version: </span><span style=color:#e6db74>{</span>torch_version<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;transformers version: </span><span style=color:#e6db74>{</span>transformers_version<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;wandb version: </span><span style=color:#e6db74>{</span>wandb_version<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;trl version: </span><span style=color:#e6db74>{</span>trl_version<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;peft version: </span><span style=color:#e6db74>{</span>peft_version<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;datasets version: </span><span style=color:#e6db74>{</span>datasets_version<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><h3 id=step-2-check-intel-xpu-availability-and-retrieve-device-capabilities>Step 2: Check Intel XPU Availability and Retrieve Device Capabilities</h3><p>In this step, we will import necessary libraries, check the availability of Intel XPU (eXtreme Performance Unit), and retrieve detailed device capabilities.
This ensures that our environment is correctly configured to leverage the Intel XPU optimal performance.</p><p>To optimize performance when using Intel Max Series GPUs:</p><ol><li><strong>Retrieve CPU Information</strong>: Determine the number of physical CPU cores and calculate cores per socket using <code>psutil</code>.</li><li><strong>Set Environment Variables</strong>:<ul><li>Disable tokenizers parallelism.</li><li>Improve memory allocation with <code>LD_PRELOAD</code> (optional).</li><li>Reduce GPU command submission overhead.</li><li>Enable SDP fusion for efficient memory usage.</li><li>Configure OpenMP to use physical cores, bind threads, and set thread pinning.</li></ul></li><li><strong>Print Configuration</strong>: Display the number of physical cores, cores per socket, and OpenMP environment variables to verify the settings.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> intel_extension_for_pytorch <span style=color:#66d9ef>as</span> ipex
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> warnings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>warnings<span style=color:#f92672>.</span>filterwarnings(<span style=color:#e6db74>&#34;ignore&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check if Intel XPU is available</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Intel XPU is available&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>device_count()):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU Device </span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_name(i)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Get the device capability details</span>
</span></span><span style=display:flex><span>    device_capability <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_capability()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Convert the device capability details to a JSON string with indentation for readability</span>
</span></span><span style=display:flex><span>    readable_device_capability <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>dumps(device_capability, indent<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print the readable JSON</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Detail of GPU capability =</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, readable_device_capability)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Intel XPU is not available&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_physical_cores <span style=color:#f92672>=</span> psutil<span style=color:#f92672>.</span>cpu_count(logical<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>num_cores_per_socket <span style=color:#f92672>=</span> num_physical_cores <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;TOKENIZERS_PARALLELISM&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;false&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#HF_TOKEN = os.environ[&#34;HF_TOKEN&#34;]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set the LD_PRELOAD environment variable</span>
</span></span><span style=display:flex><span>ld_preload <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;LD_PRELOAD&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>conda_prefix <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;CONDA_PREFIX&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Improve memory allocation performance, if tcmalloc is not availab&gt;?le, please comment this line out</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;LD_PRELOAD&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>ld_preload<span style=color:#e6db74>}</span><span style=color:#e6db74>:</span><span style=color:#e6db74>{</span>conda_prefix<span style=color:#e6db74>}</span><span style=color:#e6db74>/lib/libtcmalloc.so&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Reduce the overhead of submitting commands to the GPU</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># reducing memory accesses by fusing SDP ops</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;ENABLE_SDP_FUSION&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># set openMP threads to number of physical cores</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;OMP_NUM_THREADS&#34;</span>] <span style=color:#f92672>=</span> str(num_physical_cores)
</span></span><span style=display:flex><span><span style=color:#75715e># Set the thread affinity policy</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;OMP_PROC_BIND&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;close&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set the places for thread pinning</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;OMP_PLACES&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;cores&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of physical cores: </span><span style=color:#e6db74>{</span>num_physical_cores<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of cores per socket: </span><span style=color:#e6db74>{</span>num_cores_per_socket<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;OpenMP environment variables:&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;  - OMP_NUM_THREADS: </span><span style=color:#e6db74>{</span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;OMP_NUM_THREADS&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;  - OMP_PROC_BIND: </span><span style=color:#e6db74>{</span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;OMP_PROC_BIND&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;  - OMP_PLACES: </span><span style=color:#e6db74>{</span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;OMP_PLACES&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><h3 id=step-2-monitor-xpu-memory-usage-in-real-time>Step 2: Monitor XPU Memory Usage in Real-Time</h3><p>The following script sets up a real-time monitoring system that continuously displays the XPU memory usage in a Jupyter notebook, helping you keep track of resource utilization during model training and inference. This setup helps in maintaining optimal performance and preventing resource-related issues during your deep learning tasks. By keeping track of memory usage, you can prevent out-of-memory errors, optimize resource allocation, and ensure smooth training and inference processes. By monitoring these metrics, you can predict out-of-memory issues. If memory usage approaches the hardware limits, it’s an indication that the model or batch size might need adjusted etc.</p><ul><li><strong>Memory Reserved</strong>: Indicates the total memory reserved by the XPU. Helps in understanding the memory footprint of the running processes.</li><li><strong>Memory Allocated</strong>: Shows the actual memory usage by tensors, crucial for identifying memory leaks or excessive usage.</li><li><strong>Max Memory Reserved/Allocated</strong>: These metrics help in identifying peak memory usage, which is essential for planning and scaling your models.</li><li>performance and preventing resource-related issues during your deep learning tasks.eemory_monitor(output)</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> psutil
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> asyncio
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> threading
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> IPython.display <span style=color:#f92672>import</span> display, HTML
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> intel_extension_for_pytorch <span style=color:#66d9ef>as</span> ipex
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available():
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>empty_cache()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_memory_usage</span>():
</span></span><span style=display:flex><span>        memory_reserved <span style=color:#f92672>=</span> round(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>memory_reserved() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span><span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        memory_allocated <span style=color:#f92672>=</span> round(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>memory_allocated() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span><span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        max_memory_reserved <span style=color:#f92672>=</span> round(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>max_memory_reserved() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span><span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        max_memory_allocated <span style=color:#f92672>=</span> round(torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>max_memory_allocated() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span><span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>print_memory_usage</span>():
</span></span><span style=display:flex><span>        device_name <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_name()
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU Name: </span><span style=color:#e6db74>{</span>device_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated <span style=color:#f92672>=</span> get_memory_usage()
</span></span><span style=display:flex><span>        memory_usage_text <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU Memory: Reserved=</span><span style=color:#e6db74>{</span>memory_reserved<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Allocated=</span><span style=color:#e6db74>{</span>memory_allocated<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Max Reserved=</span><span style=color:#e6db74>{</span>max_memory_reserved<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Max Allocated=</span><span style=color:#e6db74>{</span>max_memory_allocated<span style=color:#e6db74>}</span><span style=color:#e6db74> GB&#34;</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\r</span><span style=color:#e6db74>{</span>memory_usage_text<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span>, flush<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>display_memory_usage</span>(output):
</span></span><span style=display:flex><span>        device_name <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>get_device_name()
</span></span><span style=display:flex><span>        output<span style=color:#f92672>.</span>update(HTML(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&lt;p&gt;XPU Name: </span><span style=color:#e6db74>{</span>device_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&lt;/p&gt;&#34;</span>))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated <span style=color:#f92672>=</span> get_memory_usage()
</span></span><span style=display:flex><span>            memory_usage_text <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;XPU (</span><span style=color:#e6db74>{</span>device_name<span style=color:#e6db74>}</span><span style=color:#e6db74>) :: Memory: Reserved=</span><span style=color:#e6db74>{</span>memory_reserved<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Allocated=</span><span style=color:#e6db74>{</span>memory_allocated<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Max Reserved=</span><span style=color:#e6db74>{</span>max_memory_reserved<span style=color:#e6db74>}</span><span style=color:#e6db74> GB, Max Allocated=</span><span style=color:#e6db74>{</span>max_memory_allocated<span style=color:#e6db74>}</span><span style=color:#e6db74> GB&#34;</span>
</span></span><span style=display:flex><span>            output<span style=color:#f92672>.</span>update(HTML(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&lt;p&gt;</span><span style=color:#e6db74>{</span>memory_usage_text<span style=color:#e6db74>}</span><span style=color:#e6db74>&lt;/p&gt;&#34;</span>))
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>start_memory_monitor</span>(output):
</span></span><span style=display:flex><span>        loop <span style=color:#f92672>=</span> asyncio<span style=color:#f92672>.</span>new_event_loop()
</span></span><span style=display:flex><span>        asyncio<span style=color:#f92672>.</span>set_event_loop(loop)
</span></span><span style=display:flex><span>        loop<span style=color:#f92672>.</span>create_task(display_memory_usage(output))
</span></span><span style=display:flex><span>        thread <span style=color:#f92672>=</span> threading<span style=color:#f92672>.</span>Thread(target<span style=color:#f92672>=</span>loop<span style=color:#f92672>.</span>run_forever)
</span></span><span style=display:flex><span>        thread<span style=color:#f92672>.</span>start()    
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> display(display_id<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    start_memory_monitor(output)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;XPU device not available.&#34;</span>)
</span></span></code></pre></div><h3 id=step-3-load-and-prepare-the-model>Step 3: Load and Prepare the Model</h3><p>In this step, we ensure the model is loaded and prepared for use on the appropriate device, either an Intel XPU or CPU, and configure it for efficient fine-tuningThis ensures the model and tokenizer are properly set up and optimized for use on the selected device, ready for efficient fine-tuning.
This step ensures that the model and tokenizer are correctly set up and configured for use on the appropriate device, preparing them for the fine-tuning process.
.</p><ol><li><p><strong>Check Device Availability</strong>:</p><ul><li>Check if an XPU is available and set the device accordingly. If the XPU is available and <code>USE_CPU</code> is not set to <code>True</code>, use the XPU; otherwise, use the CPU.</li></ul></li><li><p><strong>Specify Model Name</strong>:</p><ul><li>Define the model name to be used.</li></ul></li><li><p><strong>Download Model if Not Existing Locally</strong>:</p><ul><li>Define a function to check if the model exists locally.</li><li>If the model does not exist locally, download it from the specified model name, save the tokenizer and model locally.</li></ul></li><li><p><strong>Load Model and Tokenizer</strong>:</p><ul><li>Load the model and tokenizer from the local directory where they were saved.</li><li>Set the padding token and padding side for the tokenizer.</li><li>Resize the model&rsquo;s embeddings to account for any new special tokens added.</li><li>Set the padding token ID in the model&rsquo;s generation configuration.</li></ul></li><li><p><strong>Move Model to Device</strong>:</p><ul><li>Move the model to the appropriate device (XPU or CPU).</li></ul></li><li><p><strong>Configure Model for Fine-Tuning</strong>:</p><ul><li>Disable the caching mechanism to reduce memory usage during fine-tuning.</li><li>Configure the model&rsquo;s pre-training teigured for use on the appropriate device, preparing them for the fine-tuning process.</li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check if XPU is available and set the device accordingly</span>
</span></span><span style=display:flex><span>USE_CPU <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;xpu:0&#34;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>is_available() <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> USE_CPU <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Using device: </span><span style=color:#e6db74>{</span>device<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Specify the model name</span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the model and tokenizer </span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check if the tokenizer has a padding token</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Adding padding token to tokenizer.&#34;</span>)
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>.</span>add_special_tokens({<span style=color:#e6db74>&#39;pad_token&#39;</span>: <span style=color:#e6db74>&#39;[PAD]&#39;</span>})
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>resize_token_embeddings(len(tokenizer))
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Padding token already exists: </span><span style=color:#e6db74>{</span>tokenizer<span style=color:#f92672>.</span>pad_token<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># # Set the padding token and padding side</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>or</span> tokenizer<span style=color:#f92672>.</span>eos_token
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>padding_side <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;right&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># # # Resize the model embeddings to account for the new special tokens</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>resize_token_embeddings(len(tokenizer))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># # Debugging statements</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Padding token: </span><span style=color:#e6db74>{</span>tokenizer<span style=color:#f92672>.</span>pad_token<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Padding token ID: </span><span style=color:#e6db74>{</span>tokenizer<span style=color:#f92672>.</span>pad_token_id<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set the padding token ID for the generation configuration</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>generation_config<span style=color:#f92672>.</span>pad_token_id <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>pad_token_id
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Move the model to the appropriate device</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Disable caching mechanism to reduce memory usage during fine-tuning</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>use_cache <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Configure the model&#39;s pre-training tensor parallelism degree</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>pretraining_tp <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Model and tokenizer are ready for use.&#34;</span>)
</span></span></code></pre></div><h2 id=step-4-log-into-your-hugging-face-account-with-your-access-token>Step 4 Log into your hugging face account with your access token.</h2><p>Uncheck the Add token as git credential! 🎛️</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#loggin to huggnigface</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> huggingface_hub <span style=color:#f92672>import</span> notebook_login
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>notebook_login()
</span></span></code></pre></div><h3 id=step-5-load-and-inspect-the-dataset->Step 5: Load and Inspect the Dataset 📊</h3><p>Import the load_dataset function and load the specified dataset from the Hugging Face datasets library. In this case, the dataset identifier is RayBernard/nvidia-dgx-best-practices,
and we are loading the training split of the dataset. Print the first instruction and response from the dataset to ensure the content is as expected.
Next, print the total number of examples in the dataset to understand its size. List the fields (keys) present in the dataset to understand its structure.</p><p>Format and Split the Dataset for Training</p><p>This step ensures your dataset is properly formatted and split for the training process, making it ready for fine-tuning.</p><ol><li><p><strong>Load and Define</strong>:</p><ul><li>Load the dataset with the specified name and split. Here, we are loading the &ldquo;train&rdquo; split of the dataset.</li><li>Define the system message to be used for formatting prompts.</li></ul></li><li><p><strong>Format Prompts</strong>:</p><ul><li>Use the <code>format_prompts</code> function to format the dataset prompts according to the Meta Llama 3 Instruct prompt template with special tokens.</li><li>This function iterates over the &lsquo;instruction&rsquo; and &lsquo;output&rsquo; fields in the batch and formats them accordingly.</li><li>Apply the <code>format_prompts</code> function to the dataset in a batched manner for efficiency.</li></ul></li><li><p><strong>Split the Dataset</strong>:</p><ul><li>Split the formatted dataset into training and validation sets, using 20% of the data for validation and setting a seed for reproducibility.</li></ul></li><li><p><strong>Verify the Split</strong>:</p><ul><li>Print the number of examples in both the training and validation sets to verify the split.</li></ul></li><li><p><strong>Show Formatted Prompt</strong>:</p><ul><li>Define and use a function to show the formatted prompt for the first record, demonstrating what the prompt looks like with the system message included.</li></ul></li></ol><p>This process ensures that your dataset is well-organized and ready for the training phase, enhancing the model&rsquo;s performance during fine-tuning.d contents.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load a specific dataset from the Hugging Face datasets library.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;RayBernard/nvidia-dgx-best-practices&#39; is the identifier for the dataset,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># and &#39;split=&#34;train&#34;&#39; specifies that we want the training split of the dataset.</span>
</span></span><span style=display:flex><span>dataset_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;RayBernard/nvidia-dgx-best-practices&#34;</span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(dataset_name, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the first instruction and response from the dataset to verify the content.</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Instruction is: </span><span style=color:#e6db74>{</span>dataset[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;instruction&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Response is: </span><span style=color:#e6db74>{</span>dataset[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;output&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the number of examples in the dataset.</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of examples in the dataset: </span><span style=color:#e6db74>{</span>len(dataset)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the fields (keys) present in the dataset.</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Fields in the dataset: </span><span style=color:#e6db74>{</span>list(dataset<span style=color:#f92672>.</span>features<span style=color:#f92672>.</span>keys())<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print the entire dataset to get an overview of its structure and contents.</span>
</span></span><span style=display:flex><span>print(dataset)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the dataset with the specified name and split</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Here, we are loading the &#34;train&#34; split of the dataset</span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(dataset_name, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the system message separately</span>
</span></span><span style=display:flex><span><span style=color:#75715e># system_message = &#34;Respond with the appropriate command only&#34;</span>
</span></span><span style=display:flex><span>system_message <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;You are a helpful AI &#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>format_prompts</span>(batch, system_msg):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Format the prompts according to the Meta Llama 3 Instruct prompt template with special tokens.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        batch (dict): A batch of data containing &#39;instruction&#39; and &#39;output&#39; fields.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        system_msg (str): The system message to be included in the prompt.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        dict: A dictionary containing the formatted prompts under the &#39;text&#39; key.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initialize an empty list to store the formatted prompts</span>
</span></span><span style=display:flex><span>    formatted_prompts <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Iterate over the &#39;instruction&#39; and &#39;output&#39; fields in the batch</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> instruction, output <span style=color:#f92672>in</span> zip(batch[<span style=color:#e6db74>&#34;instruction&#34;</span>], batch[<span style=color:#e6db74>&#34;output&#34;</span>]):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Format the prompt according to the Meta Llama 3 Instruct template with special tokens</span>
</span></span><span style=display:flex><span>        prompt <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;&lt;|startoftext|&gt;system</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>system_msg<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;user</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>instruction<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;assistant</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>output<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># Append the formatted prompt to the list</span>
</span></span><span style=display:flex><span>        formatted_prompts<span style=color:#f92672>.</span>append(prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Return the formatted prompts as a dictionary with the key &#39;text&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;text&#34;</span>: formatted_prompts}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Apply the format_prompts function to the dataset</span>
</span></span><span style=display:flex><span><span style=color:#75715e># The function is applied in a batched manner to speed up processing</span>
</span></span><span style=display:flex><span>formatted_dataset <span style=color:#f92672>=</span> dataset<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> batch: format_prompts(batch, system_message), batched<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Split the dataset into training and validation sets</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 20% of the data is used for validation, and a seed is set for reproducibility</span>
</span></span><span style=display:flex><span>split_dataset <span style=color:#f92672>=</span> formatted_dataset<span style=color:#f92672>.</span>train_test_split(test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, seed<span style=color:#f92672>=</span><span style=color:#ae81ff>99</span>)
</span></span><span style=display:flex><span>train_dataset <span style=color:#f92672>=</span> split_dataset[<span style=color:#e6db74>&#34;train&#34;</span>]
</span></span><span style=display:flex><span>validation_dataset <span style=color:#f92672>=</span> split_dataset[<span style=color:#e6db74>&#34;test&#34;</span>]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;train dataset == &#34;</span>,train_dataset)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;validation dataset ==&#34;</span>, validation_dataset)
</span></span><span style=display:flex><span><span style=color:#75715e># Print the number of examples in the training and validation sets</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of examples in the training set: </span><span style=color:#e6db74>{</span>len(train_dataset)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of examples in the validation set: </span><span style=color:#e6db74>{</span>len(validation_dataset)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Function to show what the prompt looks like for the first record with the system message</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>show_first_prompt</span>(system_msg):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Get the first record from the dataset</span>
</span></span><span style=display:flex><span>    first_instruction <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;instruction&#34;</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    first_output <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;output&#34;</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Format the first record using the provided system message</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&lt;|startoftext|&gt;system</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>system_msg<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;user</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>first_instruction<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;assistant</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>first_output<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print the original instruction and output</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Original instruction: </span><span style=color:#e6db74>{</span>first_instruction<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Original output: </span><span style=color:#e6db74>{</span>first_output<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print the formatted prompt</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Formatted prompt with system message:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>prompt<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Show what the prompt looks like for the first record with the system message</span>
</span></span><span style=display:flex><span>show_first_prompt(system_message)
</span></span></code></pre></div><h3 id=step-6-fine-tune-the-model-and-save-the-results>Step 6: Fine-Tune the Model and Save the Results</h3><ol><li><strong>Setup Imports and Configurations</strong>:</li></ol><p>In this step, we configure the LoRA (Low-Rank Adaptation) settings for efficient training of our model.
LoRA is a technique that improves the efficiency of training by reducing the number of parameters through low-rank decomposition. Here, we instantiate a LoraConfig object with specific parameters tailored to our training needs.</p><pre><code>Instantiate LoRA Configuration:
- r: Set to 64, this parameter controls the dimension of the low-rank decomposition, balancing model capacity and efficiency.
- lora_alpha: Set to 16, this scaling factor adjusts the output of the low-rank decomposition, influencing the strength of the adaptation.
- lora_dropout: Set to 0.5, this dropout rate applies regularization to the LoRA layers to prevent overfitting. A higher value increases regularization.
- bias: Set to &quot;none&quot;, indicating no bias is added to the LoRA layers.
- target_modules: Specifies the layers where the low-rank adaptation will be applied. Here, it includes &quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, and &quot;output_proj&quot;.
- task_type: Set to &quot;CAUSAL_LM&quot;, indicating that this configuration is for a causal language modeling task.
- This configuration optimizes the model's training efficiency and performance by carefully adjusting the parameters and specifying the target modules for low-rank adaptation.
</code></pre><ol start=3><li><p><strong>Set Environment Variables</strong>:</p><ul><li>Configure relevant environment variables for logging and configuration, including Weights and Biases project settings.</li></ul></li><li><p><strong>Load Datasets</strong>:</p><ul><li>Load the training and validation datasets.</li></ul></li><li><p><strong>Configure Training Parameters</strong>:</p><ul><li>Set training parameters including batch size, gradient accumulation steps, learning rate, and mixed precision training.</li></ul></li><li><p><strong>Initialize Trainer</strong>:</p><ul><li>Initialize the <code>SFTTrainer</code> with LoRA configuration, including training arguments and datasets.</li></ul></li><li><p><strong>Optimize Performance</strong>:</p><ul><li>Clear the XPU cache before starting the training process.</li></ul></li><li><p><strong>Begin Training</strong>:</p><ul><li>Start the training process.</li><li>Print a summary of the training results, including total training time and samples processed per second.</li><li>Handle any exceptions to ensure smooth execution.</li></ul></li><li><p><strong>Save the Model</strong>:</p><ul><li>Save the fine-tuned LoRA model to the specified path for future use.</li></ul></li></ol><p>This step-by-step approach ensures that the model is properly fine-tuned and ready for deployment, with optimal performance configurations and comprehensive logging for tracking with Weights and Bias.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> PeftModel
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> trl <span style=color:#f92672>import</span> SFTConfig, SFTTrainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> LoraConfig
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> wandb
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Configuration variables</span>
</span></span><span style=display:flex><span>PUSH_TO_HUB <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>  <span style=color:#75715e># Flag to determine if the model should be pushed to Hugging Face Hub</span>
</span></span><span style=display:flex><span>USE_WANDB <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>  <span style=color:#75715e># Flag to determine if Weights and Biases (WandB) should be used for logging</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Unset the LD_PRELOAD environment variable if it exists</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#39;LD_PRELOAD&#39;</span>, <span style=color:#66d9ef>None</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># LoRA (Low-Rank Adaptation) configuration for model fine-tuning</span>
</span></span><span style=display:flex><span>lora_config <span style=color:#f92672>=</span> LoraConfig(
</span></span><span style=display:flex><span>    r<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,  <span style=color:#75715e># Low-rank adaptation rank, best practice is between 32-64</span>
</span></span><span style=display:flex><span>           <span style=color:#75715e># Increase (e.g., 128) for better precision but higher memory use</span>
</span></span><span style=display:flex><span>    lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>,  <span style=color:#75715e># Scaling factor for the LoRA weights, best practice is 16-64</span>
</span></span><span style=display:flex><span>                    <span style=color:#75715e># Increase (e.g., 64) for more stable training but higher memory use</span>
</span></span><span style=display:flex><span>    lora_dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>,  <span style=color:#75715e># Dropout probability for LoRA layers, typical range is 0.1-0.3</span>
</span></span><span style=display:flex><span>                       <span style=color:#75715e># Decrease (e.g., 0.1) for better precision, increase (e.g., 0.5) for regularization</span>
</span></span><span style=display:flex><span>    bias<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;all&#34;</span>,  <span style=color:#75715e># Apply bias to all layers</span>
</span></span><span style=display:flex><span>    target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;q_proj&#34;</span>, <span style=color:#e6db74>&#34;v_proj&#34;</span>, <span style=color:#e6db74>&#34;output_proj&#34;</span>],  <span style=color:#75715e># Specific modules in the model to apply LoRA</span>
</span></span><span style=display:flex><span>    task_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;CAUSAL_LM&#34;</span>  <span style=color:#75715e># Task type: Causal Language Modeling, where the model predicts the next word in a sequence</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ID of the fine-tuned model to be pushed to Hugging Face Hub</span>
</span></span><span style=display:flex><span>finetuned_model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;RayBernard/llama-3-8B-Instruct-ft&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set TOKENIZERS_PARALLELISM environment variable to avoid parallelism warning during tokenization</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;TOKENIZERS_PARALLELISM&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;false&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set other environment variables for WandB logging and configuration</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;WANDB_NOTEBOOK_NAME&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;llama3-8B-FT-Intel-XPU.0.0.1.ipynb&#39;</span>  <span style=color:#75715e># Name of the notebook for WandB logging</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;WANDB_PROJECT&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;llama3-8b-Instruct-ft&#34;</span>  <span style=color:#75715e># WandB project name</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;WANDB_LOG_MODEL&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;checkpoint&#34;</span>  <span style=color:#75715e># WandB log model checkpoint</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;IPEX_TILE_AS_DEVICE&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;1&#34;</span>  <span style=color:#75715e># Intel XPU configuration</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Training configuration</span>
</span></span><span style=display:flex><span>num_train_samples <span style=color:#f92672>=</span> len(train_dataset)  <span style=color:#75715e># Number of training samples in the dataset</span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>  <span style=color:#75715e># Number of training examples utilized in one forward/backward pass, typical range is 2-16</span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># Increase (e.g., 4) for better memory utilization if sufficient GPU memory is available</span>
</span></span><span style=display:flex><span>gradient_accumulation_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>  <span style=color:#75715e># Number of steps to accumulate gradients before performing an optimizer step</span>
</span></span><span style=display:flex><span>                                  <span style=color:#75715e># Decrease (e.g., 8) to use more GPU memory per step, increase for less memory use</span>
</span></span><span style=display:flex><span>steps_per_epoch <span style=color:#f92672>=</span> num_train_samples <span style=color:#f92672>//</span> (batch_size <span style=color:#f92672>*</span> gradient_accumulation_steps)  <span style=color:#75715e># Number of steps per epoch</span>
</span></span><span style=display:flex><span>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>25</span>  <span style=color:#75715e># Total number of passes through the entire training dataset, typical range is 3-30</span>
</span></span><span style=display:flex><span>max_steps <span style=color:#f92672>=</span> steps_per_epoch <span style=color:#f92672>*</span> num_epochs  <span style=color:#75715e># Total number of training steps</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Finetuning for max number of steps: </span><span style=color:#e6db74>{</span>max_steps<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)  <span style=color:#75715e># Print the total number of training steps</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>print_training_summary</span>(results):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Print a summary of the training results.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    results: Training results object containing metrics.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Time: </span><span style=color:#e6db74>{</span>results<span style=color:#f92672>.</span>metrics[<span style=color:#e6db74>&#39;train_runtime&#39;</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74> .2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Samples/second: </span><span style=color:#e6db74>{</span>results<span style=color:#f92672>.</span>metrics[<span style=color:#e6db74>&#39;train_samples_per_second&#39;</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74> .2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Configuration for the supervised fine-tuning (SFT) trainer</span>
</span></span><span style=display:flex><span>training_args <span style=color:#f92672>=</span> SFTConfig(
</span></span><span style=display:flex><span>    run_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3-8b-finetuning2&#34;</span>,  <span style=color:#75715e># Unique name for this training run</span>
</span></span><span style=display:flex><span>    per_device_train_batch_size<span style=color:#f92672>=</span>batch_size,  <span style=color:#75715e># Batch size per device during training</span>
</span></span><span style=display:flex><span>    gradient_accumulation_steps<span style=color:#f92672>=</span>gradient_accumulation_steps,  <span style=color:#75715e># Steps to accumulate gradients before updating model parameters</span>
</span></span><span style=display:flex><span>    warmup_ratio<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,  <span style=color:#75715e># Ratio of total steps used for a linear warm-up of the learning rate, typical range is 0.06-0.2</span>
</span></span><span style=display:flex><span>    max_steps<span style=color:#f92672>=</span>max_steps,  <span style=color:#75715e># Maximum number of training steps to perform</span>
</span></span><span style=display:flex><span>    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>2e-5</span>,  <span style=color:#75715e># Learning rate for the optimizer, typical range is 1e-5 to 5e-5</span>
</span></span><span style=display:flex><span>                         <span style=color:#75715e># Decrease (e.g., 1e-5) for better precision, increase (e.g., 3e-5) for faster convergence</span>
</span></span><span style=display:flex><span>    lr_scheduler_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cosine&#34;</span>,  <span style=color:#75715e># Learning rate scheduler type: cosine annealing</span>
</span></span><span style=display:flex><span>    evaluation_strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;steps&#34;</span>,  <span style=color:#75715e># Evaluation strategy: evaluate the model every few steps</span>
</span></span><span style=display:flex><span>    save_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>,  <span style=color:#75715e># Save a checkpoint of the model every 500 steps</span>
</span></span><span style=display:flex><span>    fp16<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,  <span style=color:#75715e># Use 16-bit (half precision) floating point arithmetic to speed up training and reduce memory usage</span>
</span></span><span style=display:flex><span>    logging_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,  <span style=color:#75715e># Log training metrics every 100 steps</span>
</span></span><span style=display:flex><span>    output_dir<span style=color:#f92672>=</span>finetuned_model_id,  <span style=color:#75715e># Directory where the model checkpoints will be saved</span>
</span></span><span style=display:flex><span>    hub_model_id<span style=color:#f92672>=</span>finetuned_model_id <span style=color:#66d9ef>if</span> PUSH_TO_HUB <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>,  <span style=color:#75715e># Model ID for pushing to Hugging Face Hub</span>
</span></span><span style=display:flex><span>    report_to<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;wandb&#34;</span> <span style=color:#66d9ef>if</span> USE_WANDB <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>,  <span style=color:#75715e># Report metrics to WandB if enabled</span>
</span></span><span style=display:flex><span>    push_to_hub<span style=color:#f92672>=</span>PUSH_TO_HUB,  <span style=color:#75715e># Flag to push the model to Hugging Face Hub</span>
</span></span><span style=display:flex><span>    max_grad_norm<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>,  <span style=color:#75715e># Maximum gradient norm for gradient clipping, typical range is 0.1-1.0</span>
</span></span><span style=display:flex><span>    weight_decay<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>,  <span style=color:#75715e># Weight decay coefficient for regularization, typical range is 0.01-0.1</span>
</span></span><span style=display:flex><span>                        <span style=color:#75715e># Increase (e.g., 0.1) for more regularization to prevent overfitting, decrease for less</span>
</span></span><span style=display:flex><span>    group_by_length<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,  <span style=color:#75715e># Group sequences of similar length for efficient training</span>
</span></span><span style=display:flex><span>    gradient_checkpointing<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>  <span style=color:#75715e># Enable gradient checkpointing to save memory at the cost of some compute overhead</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize the SFT trainer with the model, training arguments, datasets, and tokenizer</span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model,  <span style=color:#75715e># Model to be fine-tuned</span>
</span></span><span style=display:flex><span>    args<span style=color:#f92672>=</span>training_args,  <span style=color:#75715e># Training arguments</span>
</span></span><span style=display:flex><span>    train_dataset<span style=color:#f92672>=</span>train_dataset,  <span style=color:#75715e># Training dataset</span>
</span></span><span style=display:flex><span>    eval_dataset<span style=color:#f92672>=</span>validation_dataset,  <span style=color:#75715e># Validation dataset</span>
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>=</span>tokenizer,  <span style=color:#75715e># Tokenizer</span>
</span></span><span style=display:flex><span>    peft_config<span style=color:#f92672>=</span>lora_config,  <span style=color:#75715e># LoRA configuration</span>
</span></span><span style=display:flex><span>    dataset_text_field<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;text&#34;</span>,  <span style=color:#75715e># Text field in the dataset</span>
</span></span><span style=display:flex><span>    max_seq_length<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>,  <span style=color:#75715e># Maximum sequence length for tokenization, typical values are 512-1024</span>
</span></span><span style=display:flex><span>                         <span style=color:#75715e># Increase (e.g., 1024) for handling longer contexts but higher memory use</span>
</span></span><span style=display:flex><span>    packing<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>  <span style=color:#75715e># Enable packing of sequences to make use of available space in a batch more efficiently</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Try to train the model and handle any exceptions</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>xpu<span style=color:#f92672>.</span>empty_cache()  <span style=color:#75715e># Clear the cache of the Intel XPU</span>
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> trainer<span style=color:#f92672>.</span>train()  <span style=color:#75715e># Train the model</span>
</span></span><span style=display:flex><span>    print_training_summary(results)  <span style=color:#75715e># Print a summary of the training results</span>
</span></span><span style=display:flex><span>    wandb<span style=color:#f92672>.</span>finish()  <span style=color:#75715e># Finish the WandB run</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error during training: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)  <span style=color:#75715e># Print any errors that occur during training</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save the fine-tuned model</span>
</span></span><span style=display:flex><span>tuned_lora_model <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;llama3-8b-Instruct-ft-lora&#34;</span>  <span style=color:#75715e># Directory name for saving the model</span>
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>save_pretrained(tuned_lora_model)  <span style=color:#75715e># Save the model to the specified directory</span>
</span></span></code></pre></div><h3 id=step-12-merge-and-save-the-fine-tuned-model>Step 12: Merge and Save the Fine-Tuned Model</h3><p>After fine-tuning the model, merge the fine-tuned LoRA model with the base model and save the final tuned model. This process ensures that the fine-tuning adjustments are integrated into the base model, resulting in an optimized and ready-to-use model.</p><ol><li><strong>Import Required Libraries</strong>: Import the necessary libraries from <code>peft</code> and <code>transformers</code>.</li><li><strong>Load Base Model</strong>: Load the base model using <code>AutoModelForCausalLM</code> with the specified model ID and configurations to optimize memory usage and performance.</li><li><strong>Merge Models</strong>: Use <code>PeftModel</code> to load the fine-tuned LoRA model and merge it with the base model.</li><li><strong>Unload Unnecessary Parameters</strong>: Merge and unload unnecessary parameters from the model to optimize it.</li><li><strong>Save the Final Model</strong>: Save the final merged model to the specified path for future use.</li></ol><p>This step finalizes the training process by producing a single, fine-tuned model-ready inferencing.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Import the necessary libraries</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> PeftModel
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the path to the fine-tuned LoRA model and the base model ID</span>
</span></span><span style=display:flex><span>tuned_lora_model <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;llama3-8b-Instruct-ft-lora&#34;</span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the base model using the specified model ID.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># The parameters used are:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - low_cpu_mem_usage: Reduces memory usage on the CPU.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - return_dict: Ensures the model returns outputs as a dictionary.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - torch_dtype: Specifies the data type as bfloat16 for efficient computation.</span>
</span></span><span style=display:flex><span>base_model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    model_id,
</span></span><span style=display:flex><span>    low_cpu_mem_usage<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    return_dict<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bfloat16,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the tokenizer using the same model ID</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check if the tokenizer has a padding token</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Adding padding token to tokenizer.&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Add a special padding token to the tokenizer if it doesn&#39;t have one</span>
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>.</span>add_special_tokens({<span style=color:#e6db74>&#39;pad_token&#39;</span>: <span style=color:#e6db74>&#39;[PAD]&#39;</span>})
</span></span><span style=display:flex><span>    <span style=color:#75715e># Resize the token embeddings of the base model to match the updated tokenizer</span>
</span></span><span style=display:flex><span>    base_model<span style=color:#f92672>.</span>resize_token_embeddings(len(tokenizer))
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Padding token already exists: </span><span style=color:#e6db74>{</span>tokenizer<span style=color:#f92672>.</span>pad_token<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Ensure the padding token is set (use the existing one or the EOS token if none exists)</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>or</span> tokenizer<span style=color:#f92672>.</span>eos_token
</span></span><span style=display:flex><span><span style=color:#75715e># Set the padding side to &#39;right&#39;, meaning that padding tokens will be added to the end of sequences</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>padding_side <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;right&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the PEFT (Parameter-Efficient Fine-Tuning) model with the pre-trained base model and the LoRA-tuned model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> PeftModel<span style=color:#f92672>.</span>from_pretrained(base_model, tuned_lora_model)
</span></span><span style=display:flex><span><span style=color:#75715e># Merge the LoRA parameters into the base model and unload the PEFT structure</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>merge_and_unload()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the path where the final tuned model and tokenizer will be saved</span>
</span></span><span style=display:flex><span>final_model_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;final-tuned-model&#34;</span>  <span style=color:#75715e># Replace with your desired path</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save the final tuned model to the specified path</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>save_pretrained(final_model_path)
</span></span><span style=display:flex><span><span style=color:#75715e># Save the tokenizer to the specified path</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>save_pretrained(final_model_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Final tuned model and tokenizer saved successfully.&#34;</span>)
</span></span></code></pre></div><h3 id=optional-upload-your-model-to-hugging-face-hub>Optional Upload your model to hugging face hub</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> huggingface_hub <span style=color:#f92672>import</span> HfApi, upload_folder, login
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>final_model_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;final-tuned-model&#34;</span>  <span style=color:#75715e># Path where the model and tokenizer are saved</span>
</span></span><span style=display:flex><span>repo_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;llama3-8b-Instruct-finetuned&#34;</span>  <span style=color:#75715e># Name of the repository on Hugging Face Hub</span>
</span></span><span style=display:flex><span>username <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;RayBernard&#34;</span>  <span style=color:#75715e># Your username</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># # Log in to your Hugging Face account and get the token</span>
</span></span><span style=display:flex><span><span style=color:#75715e># login()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Retrieve the token from the cache</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>expanduser(<span style=color:#e6db74>&#34;~/.cache/huggingface/token&#34;</span>), <span style=color:#e6db74>&#34;r&#34;</span>) <span style=color:#66d9ef>as</span> token_file:
</span></span><span style=display:flex><span>    token <span style=color:#f92672>=</span> token_file<span style=color:#f92672>.</span>read()<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create a new repository or use an existing one</span>
</span></span><span style=display:flex><span>api <span style=color:#f92672>=</span> HfApi()
</span></span><span style=display:flex><span>api<span style=color:#f92672>.</span>create_repo(repo_id<span style=color:#f92672>=</span>repo_name, token<span style=color:#f92672>=</span>token, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Upload the entire model directory to the repository</span>
</span></span><span style=display:flex><span>upload_folder(
</span></span><span style=display:flex><span>    folder_path<span style=color:#f92672>=</span>final_model_path,
</span></span><span style=display:flex><span>    repo_id<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>username<span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>{</span>repo_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>    token<span style=color:#f92672>=</span>token,
</span></span><span style=display:flex><span>    repo_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Model and tokenizer uploaded to Hugging Face Hub repository: </span><span style=color:#e6db74>{</span>repo_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><h3 id=test-model-without-fine-tunning--note-at-this-point-you-should-restart-the-kernel-and-clear-so-resources-are-freed-up>Test Model without fine tunning ** Note at this point you should restart the kernel and clear so resources are freed up</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Import necessary libraries</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> transformers
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the model ID for the base model</span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create a text-generation pipeline using the specified model ID</span>
</span></span><span style=display:flex><span>pipeline <span style=color:#f92672>=</span> transformers<span style=color:#f92672>.</span>pipeline(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;text-generation&#34;</span>,             <span style=color:#75715e># Task type is text generation</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model_id,                <span style=color:#75715e># Use the specified model</span>
</span></span><span style=display:flex><span>    model_kwargs<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;torch_dtype&#34;</span>: torch<span style=color:#f92672>.</span>bfloat16},  <span style=color:#75715e># Model parameters: use bfloat16 for efficient computation</span>
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,             <span style=color:#75715e># Automatically map model to available devices (e.g., GPU if available)</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define a list of messages for the text-generation pipeline</span>
</span></span><span style=display:flex><span>messages_list <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Just respond with the command&#34;</span>},  <span style=color:#75715e># System message</span>
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;how many gpu are in an h100?&#34;</span>},      <span style=color:#75715e># User query</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Just respond with the command&#34;</span>},  <span style=color:#75715e># System message</span>
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;how many gpu are in an h200?&#34;</span>},      <span style=color:#75715e># User query</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Just respond with the command&#34;</span>},  <span style=color:#75715e># System message</span>
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;what kind of switch os to run InfiniBand network&#34;</span>},  <span style=color:#75715e># User query</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define terminators for stopping the generation</span>
</span></span><span style=display:flex><span>terminators <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    pipeline<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>eos_token_id,  <span style=color:#75715e># End of sequence token ID</span>
</span></span><span style=display:flex><span>    pipeline<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>convert_tokens_to_ids(<span style=color:#e6db74>&#34;&#34;</span>)  <span style=color:#75715e># Convert an empty string to a token ID (might be unnecessary)</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span><span style=color:#75715e># Loop through the list of messages</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> messages <span style=color:#f92672>in</span> messages_list:
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> pipeline(
</span></span><span style=display:flex><span>        messages,                  <span style=color:#75715e># Input messages to the pipeline</span>
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>,        <span style=color:#75715e># Generate up to 256 new tokens</span>
</span></span><span style=display:flex><span>        eos_token_id<span style=color:#f92672>=</span>terminators,  <span style=color:#75715e># Use the defined terminators to stop generation</span>
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,            <span style=color:#75715e># Enable sampling for text generation</span>
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>,           <span style=color:#75715e># Sampling temperature: lower value makes output more deterministic</span>
</span></span><span style=display:flex><span>        top_p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,                 <span style=color:#75715e># Top-p (nucleus) sampling: consider the top 90% probability mass</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    print(outputs[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;generated_text&#34;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])  <span style=color:#75715e># Print the last token of the generated text</span>
</span></span></code></pre></div><h1 id=now-test-the-model-after-fine-tuning>Now test the model after fine-tuning</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> transformers
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the model ID for the fine-tuned model</span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;final-tuned-model&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create a text-generation pipeline using the fine-tuned model ID</span>
</span></span><span style=display:flex><span>pipeline <span style=color:#f92672>=</span> transformers<span style=color:#f92672>.</span>pipeline(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;text-generation&#34;</span>,             <span style=color:#75715e># Task type is text generation</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model_id,                <span style=color:#75715e># Use the fine-tuned model</span>
</span></span><span style=display:flex><span>    model_kwargs<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;torch_dtype&#34;</span>: torch<span style=color:#f92672>.</span>bfloat16},  <span style=color:#75715e># Model parameters: use bfloat16 for efficient computation</span>
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,             <span style=color:#75715e># Automatically map model to available devices (e.g., GPU if available)</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the same list of messages for testing the fine-tuned model</span>
</span></span><span style=display:flex><span>messages_list <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Just respond with the command&#34;</span>},  <span style=color:#75715e># System message</span>
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;how many gpu are in an h100?&#34;</span>},      <span style=color:#75715e># User query</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Just respond with the command&#34;</span>},  <span style=color:#75715e># System message</span>
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;how many gpu are in an h200?&#34;</span>},      <span style=color:#75715e># User query</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Just respond with the command&#34;</span>},  <span style=color:#75715e># System message</span>
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;what kind of switch os to run InfiniBand network&#34;</span>},  <span style=color:#75715e># User query</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define terminators for stopping the generation</span>
</span></span><span style=display:flex><span>terminators <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    pipeline<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>eos_token_id,  <span style=color:#75715e># End of sequence token ID</span>
</span></span><span style=display:flex><span>    pipeline<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>convert_tokens_to_ids(<span style=color:#e6db74>&#34;&#34;</span>)  <span style=color:#75715e># Convert an empty string to a token ID (might be unnecessary)</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Loop through the list of messages</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> messages <span style=color:#f92672>in</span> messages_list:
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> pipeline(
</span></span><span style=display:flex><span>        messages,                  <span style=color:#75715e># Input messages to the pipeline</span>
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>,        <span style=color:#75715e># Generate up to 256 new tokens</span>
</span></span><span style=display:flex><span>        eos_token_id<span style=color:#f92672>=</span>terminators,  <span style=color:#75715e># Use the defined terminators to stop generation</span>
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,            <span style=color:#75715e># Enable sampling for text generation</span>
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,           <span style=color:#75715e># Sampling temperature: lower value makes output more deterministic</span>
</span></span><span style=display:flex><span>        top_p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,                 <span style=color:#75715e># Top-p (nucleus) sampling: consider the top 90% probability mass</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    print(outputs[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;generated_text&#34;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])  <span style=color:#75715e># Print the last token of the generated text</span>
</span></span></code></pre></div><h2 id=happy-fine-tuning->Happy Fine-Tuning! 😄✨</h2><p>Congratulations on reaching this exciting milestone! You now possess the tools and knowledge to fine-tune the powerful LLaMA 3 model on your own custom datasets. This achievement opens up a world of possibilities for you to explore and unleash the full potential of this cutting-edge language model.
We encourage you to embrace the spirit of experimentation and exploration. Feel free to customize and adapt this notebook to fit your specific use case. Try different datasets, tweak the hyperparameters, and observe how the model&rsquo;s performance evolves. This hands-on experience will deepen your understanding and allow you to tailor the model to your unique requirements.
Moreover, we invite you to share your fine-tuned models and experiences with the broader community. Consider open-sourcing your work on platforms like GitHub or Hugging Face, and write blog posts to detail your fine-tuning journey. Your insights and achievements can inspire and assist others who are embarking on their own fine-tuning projects, fostering a collaborative and supportive environment for knowledge sharing.
If you encounter any challenges or have suggestions for improvement, please don&rsquo;t hesitate to reach out and provide feedback. We value your input and are committed to making this notebook and the fine-tuning process as smooth and enjoyable as possible. Your feedback will help us refine and enhance the resources available to the community.
Remember, the journey of fine-tuning language models is an iterative and continuous process. Embrace the challenges, celebrate your successes, and continue pushing the boundaries of what&rsquo;s possible. Together, we can unlock the full potential of these powerful models and drive innovation in various domains.</p><h2 id=star-my-repo-please>Star my repo please</h2><p><a href=https://github.com/raymondbernard/>Star this Repo on GitHub</a></p><h2 id=download>Download</h2><p>You can download the Intel Notebook by clicking the link below:</p><a href=/llama3-8B-FT-Intel-XPU.0.0.3.ipynb download=llama3-8B-FT-Intel-XPU.0.0.3.ipynb>Download llama3-8B-FT-Intel-XPU.0.0.3.ipynb</a>
<script>document.getElementById("").addEventListener("click",function(e){e.preventDefault();const t=document.createElement("a");t.href=this.href,t.download="llama3-8B-FT-Intel-XPU.0.0.3.ipynb",document.body.appendChild(t),t.click(),document.body.removeChild(t)})</script><h2 id=comments>Comments</h2><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://raymondbernard-github-io.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><p>SPDX-License-Identifier: Apache-2.0
Copyright (c) 2024, Rahul Unnikrishnan Nair <a href=mailto:rahul.unnikrishnan.nair@intel.com>rahul.unnikrishnan.nair@intel.com</a>
Ray Bernard <a href=mailto:ray.bernard@outlookcom>ray.bernard@outlookcom</a></p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=http://localhost:1313/>&copy; Ray Bernard's Fine Tuned Journal 2024</a><div><div class=ananke-socials><a href=https://www.github.com/raymondbernard/fine-tune-journal target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="github link" aria-label="follow on github——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.linkedin.com/in/raymond-bernard-960382/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="My Linkedin link" aria-label="follow on My Linkedin——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q target=_blank rel=noopener class="youtube ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="youtube link" aria-label="follow on youtube——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527 41.34c-.278.0-.478.078-.6.244-.121.156-.18.424-.18.796v.896h1.543V42.38c0-.372-.062-.64-.185-.796C42.989 41.418 42.792 41.34 42.527 41.34zM36.509 41.309c.234.0.417.076.544.23.123.155.185.383.185.682v4.584c0 .286-.053.487-.153.611-.1.127-.256.189-.47.189-.148.0-.287-.033-.421-.096-.135-.062-.274-.171-.415-.313v-5.531c.119-.122.239-.213.36-.271C36.26 41.335 36.383 41.309 36.509 41.309zm5.239 3.349v1.672c0 .468.057.792.17.974.118.181.313.269.592.269.289.0.491-.076.606-.229.114-.153.175-.489.175-1.013v-.405h1.795v.456c0 .911-.217 1.596-.657 2.059-.435.459-1.089.687-1.958.687-.781.0-1.398-.242-1.847-.731-.448-.486-.676-1.157-.676-2.014v-3.986c0-.768.249-1.398.742-1.882.493-.484 1.128-.727 1.911-.727.799.0 1.413.225 1.843.674.429.448.642 1.093.642 1.935v2.264H41.748zm-3.125 3.837c-.271.336-.669.501-1.187.501-.343.0-.646-.062-.912-.192-.267-.129-.519-.327-.746-.601v.681h-1.764V36.852h1.764v3.875c.237-.27.485-.478.748-.616.267-.143.534-.212.805-.212.554.0.975.189 1.265.565.294.379.438.933.438 1.66v4.926C39.034 47.678 38.897 48.159 38.623 48.495zM30.958 48.884v-.976c-.325.361-.658.636-1.009.822-.349.191-.686.282-1.014.282-.405.0-.705-.129-.913-.396-.201-.266-.305-.658-.305-1.189v-7.422h1.744v6.809c0 .211.037.362.107.457.077.095.196.141.358.141.128.0.292-.062.488-.188.197-.125.375-.283.542-.475v-6.744H32.7v8.878H30.958zM24.916 38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916zm8.078-5.622c0-.001 12.08.018 13.514 1.45 1.439 1.435 1.455 8.514 1.455 8.555.0.0-.012 7.117-1.455 8.556C45.074 52.969 32.994 53 32.994 53s-12.079-.031-13.516-1.462c-1.438-1.435-1.441-8.502-1.441-8.556.0-.041.004-7.12 1.441-8.555 1.438-1.431 13.516-1.45 13.516-1.449zm9.526-3.723h-1.966v-1.08c-.358.397-.736.703-1.13.909-.392.208-.771.312-1.14.312-.458.0-.797-.146-1.027-.437-.229-.291-.345-.727-.345-1.311v-8.172h1.962v7.497c0 .231.045.399.127.502.08.104.216.156.399.156.143.0.327-.069.548-.206.22-.137.423-.312.605-.527v-7.422h1.966V29.255zM31.847 27.588c.139.147.339.219.6.219.266.0.476-.075.634-.223.157-.152.235-.358.235-.618v-5.327c0-.214-.08-.387-.241-.519-.16-.131-.37-.196-.628-.196-.241.0-.435.065-.586.196-.148.132-.225.305-.225.519v5.327C31.636 27.233 31.708 27.439 31.847 27.588zm-1.439-7.685c.528-.449 1.241-.674 2.132-.674.812.0 1.48.237 2.001.711.517.473.777 1.083.777 1.828v5.051c0 .836-.255 1.491-.762 1.968-.513.476-1.212.714-2.106.714-.858.0-1.547-.246-2.064-.736-.513-.492-.772-1.152-.772-1.983v-5.068C29.613 20.954 29.877 20.351 30.408 19.903zM24.262 16h-2.229l2.634 8.003v5.252h2.213v-5.5L29.454 16h-2.25l-1.366 5.298h-.139L24.262 16zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://x.com/raybernard007 target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://medium.com/@raybernard007 target=_blank rel=noopener class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 170 170" viewBox="0 0 170 170" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M46.5340803 65.2157554C46.6968378 63.6076572 46.0836 62.018231 44.8828198 60.93592L32.6512605 46.2010582V44H70.6302521l29.3557423 64.380952L125.794585 44H162v2.2010582L151.542017 56.2281011C150.640424 56.9153477 150.193188 58.0448862 150.380019 59.1628454V132.837155C150.193188 133.955114 150.640424 135.084652 151.542017 135.771899l10.213352 10.027043V148H110.38282V145.798942l10.580299-10.271605c1.039682-1.039389 1.039682-1.345091 1.039682-2.934744V73.0417402l-29.4169 74.7136978H88.6106443L54.3622782 73.0417402V123.115814C54.0767278 125.221069 54.7759199 127.3406 56.2581699 128.863022L70.0186741 145.55438V147.755438H31V145.55438l13.7605042-16.691358c1.4714579-1.524946 2.1298796-3.658537 1.7735761-5.747208V65.2157554z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.reddit.com/user/OpenAITutor/ target=_blank rel=noopener class="reddit ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="reddit link" aria-label="follow on reddit——Opens in a new window">reddit
<span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>