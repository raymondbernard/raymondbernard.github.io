<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Open-Webui Mixture of Agents part 2 | Ray Bernard's Fine-Tuned Journal</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Youtube Video : https://youtu.be/KxT7lHaPDJ4
Introduction The Mixture-of-Agents (MoA) methodology has demonstrated state-of-the-art performance using open-source models, as detailed in my previous blog. We have created two pipelines (Groq and Ollama) for Open-WebUI. These pipelines serve as versatile, UI-agnostic OpenAI-compatible plugin frameworks.
In this blog, we will demonstrate how MoA can be integrated into Open WebUI, an extensible, feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. Open WebUI supports various LLM runners, including Ollama and OpenAI-compatible APIs."><meta name=generator content="Hugo 0.131.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link rel=canonical href=http://localhost:1313/posts/open-webui-moa-groq/><meta property="og:url" content="http://localhost:1313/posts/open-webui-moa-groq/"><meta property="og:site_name" content="Ray Bernard's Fine-Tuned Journal"><meta property="og:title" content="Open-Webui Mixture of Agents part 2"><meta property="og:description" content="Youtube Video : https://youtu.be/KxT7lHaPDJ4
Introduction The Mixture-of-Agents (MoA) methodology has demonstrated state-of-the-art performance using open-source models, as detailed in my previous blog. We have created two pipelines (Groq and Ollama) for Open-WebUI. These pipelines serve as versatile, UI-agnostic OpenAI-compatible plugin frameworks.
In this blog, we will demonstrate how MoA can be integrated into Open WebUI, an extensible, feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. Open WebUI supports various LLM runners, including Ollama and OpenAI-compatible APIs."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-03T12:00:00+00:00"><meta property="article:modified_time" content="2024-07-03T12:00:00+00:00"><meta itemprop=name content="Open-Webui Mixture of Agents part 2"><meta itemprop=description content="Youtube Video : https://youtu.be/KxT7lHaPDJ4
Introduction The Mixture-of-Agents (MoA) methodology has demonstrated state-of-the-art performance using open-source models, as detailed in my previous blog. We have created two pipelines (Groq and Ollama) for Open-WebUI. These pipelines serve as versatile, UI-agnostic OpenAI-compatible plugin frameworks.
In this blog, we will demonstrate how MoA can be integrated into Open WebUI, an extensible, feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. Open WebUI supports various LLM runners, including Ollama and OpenAI-compatible APIs."><meta itemprop=datePublished content="2024-07-03T12:00:00+00:00"><meta itemprop=dateModified content="2024-07-03T12:00:00+00:00"><meta itemprop=wordCount content="1197"><meta name=twitter:card content="summary"><meta name=twitter:title content="Open-Webui Mixture of Agents part 2"><meta name=twitter:description content="Youtube Video : https://youtu.be/KxT7lHaPDJ4
Introduction The Mixture-of-Agents (MoA) methodology has demonstrated state-of-the-art performance using open-source models, as detailed in my previous blog. We have created two pipelines (Groq and Ollama) for Open-WebUI. These pipelines serve as versatile, UI-agnostic OpenAI-compatible plugin frameworks.
In this blog, we will demonstrate how MoA can be integrated into Open WebUI, an extensible, feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. Open WebUI supports various LLM runners, including Ollama and OpenAI-compatible APIs."><script>(function(e,t,n,s,o,i){e.hj=e.hj||function(){(e.hj.q=e.hj.q||[]).push(arguments)},e._hjSettings={hjid:5009664,hjsv:6},o=t.getElementsByTagName("head")[0],i=t.createElement("script"),i.async=1,i.src=n+e._hjSettings.hjid+s+e._hjSettings.hjsv,o.appendChild(i)})(window,document,"https://static.hotjar.com/c/hotjar-",".js?sv=")</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-XT709SJYMZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XT709SJYMZ")</script></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Ray Bernard's Fine-Tuned Journal</a><div class="flex-l items-center"><div class=ananke-socials><a href=https://www.github.com/raymondbernard/fine-tune-journal target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="github link" aria-label="follow on github——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.linkedin.com/in/raymond-bernard-960382/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="My Linkedin link" aria-label="follow on My Linkedin——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q target=_blank rel=noopener class="youtube ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="youtube link" aria-label="follow on youtube——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527 41.34c-.278.0-.478.078-.6.244-.121.156-.18.424-.18.796v.896h1.543V42.38c0-.372-.062-.64-.185-.796C42.989 41.418 42.792 41.34 42.527 41.34zM36.509 41.309c.234.0.417.076.544.23.123.155.185.383.185.682v4.584c0 .286-.053.487-.153.611-.1.127-.256.189-.47.189-.148.0-.287-.033-.421-.096-.135-.062-.274-.171-.415-.313v-5.531c.119-.122.239-.213.36-.271C36.26 41.335 36.383 41.309 36.509 41.309zm5.239 3.349v1.672c0 .468.057.792.17.974.118.181.313.269.592.269.289.0.491-.076.606-.229.114-.153.175-.489.175-1.013v-.405h1.795v.456c0 .911-.217 1.596-.657 2.059-.435.459-1.089.687-1.958.687-.781.0-1.398-.242-1.847-.731-.448-.486-.676-1.157-.676-2.014v-3.986c0-.768.249-1.398.742-1.882.493-.484 1.128-.727 1.911-.727.799.0 1.413.225 1.843.674.429.448.642 1.093.642 1.935v2.264H41.748zm-3.125 3.837c-.271.336-.669.501-1.187.501-.343.0-.646-.062-.912-.192-.267-.129-.519-.327-.746-.601v.681h-1.764V36.852h1.764v3.875c.237-.27.485-.478.748-.616.267-.143.534-.212.805-.212.554.0.975.189 1.265.565.294.379.438.933.438 1.66v4.926C39.034 47.678 38.897 48.159 38.623 48.495zM30.958 48.884v-.976c-.325.361-.658.636-1.009.822-.349.191-.686.282-1.014.282-.405.0-.705-.129-.913-.396-.201-.266-.305-.658-.305-1.189v-7.422h1.744v6.809c0 .211.037.362.107.457.077.095.196.141.358.141.128.0.292-.062.488-.188.197-.125.375-.283.542-.475v-6.744H32.7v8.878H30.958zM24.916 38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916zm8.078-5.622c0-.001 12.08.018 13.514 1.45 1.439 1.435 1.455 8.514 1.455 8.555.0.0-.012 7.117-1.455 8.556C45.074 52.969 32.994 53 32.994 53s-12.079-.031-13.516-1.462c-1.438-1.435-1.441-8.502-1.441-8.556.0-.041.004-7.12 1.441-8.555 1.438-1.431 13.516-1.45 13.516-1.449zm9.526-3.723h-1.966v-1.08c-.358.397-.736.703-1.13.909-.392.208-.771.312-1.14.312-.458.0-.797-.146-1.027-.437-.229-.291-.345-.727-.345-1.311v-8.172h1.962v7.497c0 .231.045.399.127.502.08.104.216.156.399.156.143.0.327-.069.548-.206.22-.137.423-.312.605-.527v-7.422h1.966V29.255zM31.847 27.588c.139.147.339.219.6.219.266.0.476-.075.634-.223.157-.152.235-.358.235-.618v-5.327c0-.214-.08-.387-.241-.519-.16-.131-.37-.196-.628-.196-.241.0-.435.065-.586.196-.148.132-.225.305-.225.519v5.327C31.636 27.233 31.708 27.439 31.847 27.588zm-1.439-7.685c.528-.449 1.241-.674 2.132-.674.812.0 1.48.237 2.001.711.517.473.777 1.083.777 1.828v5.051c0 .836-.255 1.491-.762 1.968-.513.476-1.212.714-2.106.714-.858.0-1.547-.246-2.064-.736-.513-.492-.772-1.152-.772-1.983v-5.068C29.613 20.954 29.877 20.351 30.408 19.903zM24.262 16h-2.229l2.634 8.003v5.252h2.213v-5.5L29.454 16h-2.25l-1.366 5.298h-.139L24.262 16zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://x.com/raybernard007 target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://medium.com/@raybernard007 target=_blank rel=noopener class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 170 170" viewBox="0 0 170 170" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M46.5340803 65.2157554C46.6968378 63.6076572 46.0836 62.018231 44.8828198 60.93592L32.6512605 46.2010582V44H70.6302521l29.3557423 64.380952L125.794585 44H162v2.2010582L151.542017 56.2281011C150.640424 56.9153477 150.193188 58.0448862 150.380019 59.1628454V132.837155C150.193188 133.955114 150.640424 135.084652 151.542017 135.771899l10.213352 10.027043V148H110.38282V145.798942l10.580299-10.271605c1.039682-1.039389 1.039682-1.345091 1.039682-2.934744V73.0417402l-29.4169 74.7136978H88.6106443L54.3622782 73.0417402V123.115814C54.0767278 125.221069 54.7759199 127.3406 56.2581699 128.863022L70.0186741 145.55438V147.755438H31V145.55438l13.7605042-16.691358c1.4714579-1.524946 2.1298796-3.658537 1.7735761-5.747208V65.2157554z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.facebook.com/groups/1994768487609019 target=_blank rel=noopener class="facebook ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="reddit link" aria-label="follow on reddit——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/posts/open-webui-moa-groq/&amp;title=Open-Webui%20Mixture%20of%20Agents%20part%202" class="ananke-social-link linkedin no-underline" aria-label="share on My Linkedin"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href="https://twitter.com/intent/tweet?url=http://localhost:1313/posts/open-webui-moa-groq/&amp;text=Open-Webui%20Mixture%20of%20Agents%20part%202" class="ananke-social-link twitter no-underline" aria-label="share on twitter"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href="https://www.facebook.com/sharer.php?u=http://localhost:1313/posts/open-webui-moa-groq/" class="ananke-social-link facebook no-underline" aria-label="share on reddit"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div><h1 class="f1 athelas mt3 mb1">Open-Webui Mixture of Agents part 2</h1><time class="f6 mv4 dib tracked" datetime=2024-07-03T12:00:00Z>July 3, 2024</time>
<span class="f6 mv4 dib tracked">- 6 minutes read </span><span class="f6 mv4 dib tracked">- 1197 words</span></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Youtube Video : <a href=https://youtu.be/KxT7lHaPDJ4>https://youtu.be/KxT7lHaPDJ4</a></p><h2 id=introduction>Introduction</h2><p>The Mixture-of-Agents (MoA) methodology has demonstrated state-of-the-art performance using open-source models, as detailed in my previous blog. We have created two pipelines (Groq and Ollama) for Open-WebUI. These pipelines serve as versatile, UI-agnostic OpenAI-compatible plugin frameworks.</p><p>In this blog, we will demonstrate how MoA can be integrated into Open WebUI, an extensible, feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. Open WebUI supports various LLM runners, including Ollama and OpenAI-compatible APIs. Our project aims to incorporate MoA into this robust platform, bringing it to state-of-the-art standards.</p><p>GitHub repository for Pipelines: <a href=https://github.com/open-webui/pipelines>https://github.com/open-webui/pipelines</a></p><p>GitHub repository for Open WebUI: <a href=https://github.com/open-webui/open-webui>https://github.com/open-webui/open-webui</a></p><h2 id=understanding-the-mixture-of-agents-moa-methodology>Understanding the Mixture-of-Agents (MoA) Methodology</h2><h3 id=definition-and-concept>Definition and Concept</h3><p>The Mixture-of-Agents (MoA) methodology is an innovative approach that leverages the strengths of multiple LLMs to enhance overall performance. Unlike traditional single-model approaches, MoA constructs a layered architecture where each layer consists of multiple LLM agents. Each agent processes the outputs from the previous layer’s agents as auxiliary information, iteratively refining the responses. This collaborative framework allows MoA to achieve state-of-the-art performance by combining the diverse capabilities of different LLMs.</p><h3 id=how-moa-differs-from-traditional-approaches>How MoA Differs from Traditional Approaches</h3><p>Traditional LLM approaches typically involve a single model trained on extensive data to handle various tasks. While effective, these models face limitations in scalability and specialization. Scaling up a single model is costly and time-consuming, often requiring retraining on massive datasets. In contrast, MoA capitalizes on the inherent strengths of multiple LLMs, distributing tasks among specialized agents and iteratively refining their outputs. This not only improves performance but also offers a cost-effective and scalable solution.</p><h2 id=the-collaborativeness-of-llms>The Collaborativeness of LLMs</h2><h3 id=concept-of-collaborativeness-in-llms>Concept of Collaborativeness in LLMs</h3><p>A key insight driving the MoA methodology is the concept of collaborativeness among LLMs. This refers to the phenomenon where LLMs generate better responses when they can reference outputs from other models. This collaborativeness is evident even when the auxiliary responses are of lower quality than what an individual LLM could produce independently. By leveraging this phenomenon, MoA enhances the overall response quality through iterative refinement and synthesis.</p><h3 id=benefits-of-collaborative-llm-responses>Benefits of Collaborative LLM Responses</h3><p>The collaborativeness of LLMs offers several benefits. Firstly, it allows for the integration of diverse perspectives and strengths, leading to more robust and comprehensive responses. Secondly, it mitigates the limitations of individual models, as the collective expertise can cover a broader range of tasks and scenarios. Lastly, this collaborative approach improves the adaptability and flexibility of LLMs, enabling them to handle complex and varied inputs more effectively.</p><h3 id=install-our--moa-pipelines-for-groq>Install our MoA pipelines for Groq</h3><p><strong>Create a folder called <code>moa</code>:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir moa
</span></span><span style=display:flex><span>cd moa
</span></span></code></pre></div><p><strong>Step 1: Install open-webui</strong></p><ol><li><p>Clone the repository and navigate into it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/open-webui/open-webui.git
</span></span><span style=display:flex><span>cd open-webui
</span></span></code></pre></div></li><li><p>Create and activate a virtual environment:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># Windows</span>
</span></span><span style=display:flex><span>py -m venv .venv
</span></span><span style=display:flex><span>.venv<span style=color:#ae81ff>\S</span>cripts<span style=color:#ae81ff>\a</span>ctivate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Linux</span>
</span></span><span style=display:flex><span>python3 -m venv venv
</span></span><span style=display:flex><span>source venv/bin/activate
</span></span></code></pre></div></li><li><p>Install the required packages:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install -r requirements.txt
</span></span></code></pre></div></li><li><p>Start open-webui:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>open-webui serve
</span></span></code></pre></div></li><li><p>Wait until it starts and go to <a href=http://localhost:8080>http://localhost:8080</a>.</p></li></ol><p><strong>Step 2: Install Pipelines</strong></p><ol><li><p>Clone the pipelines repository and navigate into it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/open-webui/pipelines.git
</span></span><span style=display:flex><span>cd pipelines
</span></span></code></pre></div></li><li><p>Install the required packages:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install -r requirements.txt
</span></span></code></pre></div></li><li><p>Create an <code>.env</code> file in the <code>pipelines</code> folder. Use the template provided and replace the placeholders with your actual keys from Groq:</p><p><a href=https://github.com/raymondbernard/moa-pipeline/blob/main/.env><code>.env</code> Template</a></p></li><li><p>Run the appropriate script for your operating system:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># Windows</span>
</span></span><span style=display:flex><span>start.bat
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Linux</span>
</span></span><span style=display:flex><span>sh start.sh
</span></span></code></pre></div></li></ol><p><strong>Step 3: Configure and Use the MoA Groq Pipeline</strong></p><ol><li><p>Go to your open-webui server at <a href=http://localhost:8080>http://localhost:8080</a>.</p></li><li><p>Install from the GitHub URL:</p><ul><li><p>Navigate to <code>http://localhost:8080/admin/settings/</code></p></li><li><p>Use the URL to pull the MoA Groq code into the pipeline:</p><p><a href=https://github.com/raymondbernard/moa-pipeline/blob/main/moa_groq.py>MoA Groq Pipeline</a></p></li></ul></li><li><p>Go to the chat and start using it! Make sure you select &ldquo;MoA Groq&rdquo; from the dropdown box.</p></li></ol><p>Our Moa Groq code accomplishes the goal of sending a prompt to 3 reference models, aggregating their responses, and then synthesizing a robust response using an aggregator agent.</p><p>Please see youtube video what installation steps:</p><h3 id=key-components-to-verify>Key Components to Verify:</h3><ol><li><strong>Sending the prompt to 3 reference models.</strong></li><li><strong>Aggregating the responses from the reference models.</strong></li><li><strong>Using an aggregator agent to synthesize the responses into a robust response.</strong></li></ol><p>Below is just a few notes on how the Moa Pipeline works.
The main config is found in your .env file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>GROQ_API_BASE_1<span style=color:#f92672>=</span>https://api.groq.com/openai/v1
</span></span><span style=display:flex><span>GROQ_API_KEY_1<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;use your own token from groq&gt;&#34;</span>
</span></span><span style=display:flex><span>GROQ_API_BASE_2<span style=color:#f92672>=</span>https://api.groq.com/openai/v1
</span></span><span style=display:flex><span>GROQ_API_KEY_2<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;use your own token from groq&gt;&#34;</span>
</span></span><span style=display:flex><span>GROQ_API_BASE_3<span style=color:#f92672>=</span>https://api.groq.com/openai/v1
</span></span><span style=display:flex><span>GROQ_API_KEY_3<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;use your own token from groq&gt;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># We are using API_BASE_ for the Aggregator</span>
</span></span><span style=display:flex><span>GROQ_API_BASE_4<span style=color:#f92672>=</span>https://api.groq.com/openai/v1
</span></span><span style=display:flex><span>GROQ_API_KEY_4<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;use your own token from groq&gt;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>GROQ_API_KEY<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;use your own token from groq&gt;&#34;</span>
</span></span><span style=display:flex><span>GROQ_DEFAULT_MAX_TOKENS<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>
</span></span><span style=display:flex><span>GROQ_DEFAULT_TEMPERATURE<span style=color:#f92672>=</span>0.9
</span></span><span style=display:flex><span>GROQ_DEFAULT_ROUNDS<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>GROQ_LAYERS<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>GROQ_AGENTS_PER_LAYER<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>GROQ_MULTITURN<span style=color:#f92672>=</span>True
</span></span><span style=display:flex><span>GROQ_MODEL_AGGREGATE<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;llama3-70b-8192&#39;</span>
</span></span><span style=display:flex><span>GROQ_MODEL_AGGREGATE_API_BASE<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>GROQ_API_BASE_1<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>GROQ_MODEL_AGGREGATE_API_KEY<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>GROQ_API_KEY_1<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_1<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;llama3-8b-8192&#39;</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_1_API_BASE<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>GROQ_API_BASE_2<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_1_API_KEY<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>GROQ_API_KEY_2<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_2<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gemma-7b-it&#39;</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_2_API_BASE<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>GROQ_API_BASE_3<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_2_API_KEY<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>GROQ_API_KEY_3<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_3<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mixtral-8x7b-32768&#39;</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_3_API_BASE<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>GROQ_API_BASE_4<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>GROQ_MODEL_REFERENCE_3_API_KEY<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>GROQ_API_KEY_4<span style=color:#e6db74>}</span>
</span></span></code></pre></div><p>Note the .env will serve both pipelines</p><h3 id=code-analysis>Code Analysis:</h3><h4 id=1-sending-the-prompt-to-3-reference-models>1. Sending the Prompt to 3 Reference Models</h4><p>The prompt is sent to the 3 reference models in the <code>process_layer</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process_layer</span>(self, data, temperature<span style=color:#f92672>=</span>GROQ_DEFAULT_TEMPERATURE, max_tokens<span style=color:#f92672>=</span>GROQ_DEFAULT_MAX_TOKENS):
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Processing layer with </span><span style=color:#e6db74>{</span>len(self<span style=color:#f92672>.</span>reference_models)<span style=color:#e6db74>}</span><span style=color:#e6db74> agents&#34;</span>)
</span></span><span style=display:flex><span>    responses <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(self<span style=color:#f92672>.</span>reference_models)):
</span></span><span style=display:flex><span>        model_info <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>reference_models[self<span style=color:#f92672>.</span>current_model_index]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>rotate_agents()
</span></span><span style=display:flex><span>        logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Agent </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>: Using model </span><span style=color:#e6db74>{</span>model_info[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        response <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>process_fn(
</span></span><span style=display:flex><span>            {<span style=color:#e6db74>&#34;instruction&#34;</span>: data[<span style=color:#e6db74>&#34;instruction&#34;</span>][i]},
</span></span><span style=display:flex><span>            model_info<span style=color:#f92672>=</span>model_info,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span>temperature,
</span></span><span style=display:flex><span>            max_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        responses<span style=color:#f92672>.</span>append(response[<span style=color:#e6db74>&#34;output&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> responses
</span></span></code></pre></div><ul><li><code>process_layer</code> iterates through the <code>reference_models</code> and sends the prompt to each one using the <code>process_fn</code> function.</li><li>The responses from each reference model are collected in the <code>responses</code> list.</li></ul><h4 id=2-aggregating-the-responses-from-reference-models>2. Aggregating the Responses from Reference Models</h4><p>The responses are aggregated in the <code>aggregate_responses</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>aggregate_responses</span>(self, responses: List[str]) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    aggregated_response <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>join(responses)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> aggregated_response
</span></span></code></pre></div><ul><li>This function takes the list of responses and joins them into a single aggregated response.</li></ul><h4 id=3-using-an-aggregator-agent-to-synthesize-the-responses>3. Using an Aggregator Agent to Synthesize the Responses</h4><p>The aggregated responses are used to generate a robust response using the aggregator agent in the <code>call_aggregator_model</code> and <code>generate_with_references</code> functions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call_aggregator_model</span>(self, aggregated_responses, messages):
</span></span><span style=display:flex><span>    aggregated_message <span style=color:#f92672>=</span> [{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: aggregated_responses}]
</span></span><span style=display:flex><span>    final_response <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>generate_together(self<span style=color:#f92672>.</span>model_aggregate, aggregated_message)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> final_response
</span></span></code></pre></div><ul><li>This function sends the aggregated responses to the aggregator model and returns the synthesized response.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_with_references</span>(self, model_info, messages, references<span style=color:#f92672>=</span>[], max_tokens<span style=color:#f92672>=</span>GROQ_DEFAULT_MAX_TOKENS, temperature<span style=color:#f92672>=</span>GROQ_DEFAULT_TEMPERATURE):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(references) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        messages <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>inject_references_to_messages(messages, references)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Generating with references for model </span><span style=color:#e6db74>{</span>model_info[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>generate_together(model_info, messages<span style=color:#f92672>=</span>messages, temperature<span style=color:#f92672>=</span>temperature, max_tokens<span style=color:#f92672>=</span>max_tokens)
</span></span></code></pre></div><ul><li>The <code>generate_with_references</code> function ensures that the references are included in the messages before generating the final response.</li></ul><h3 id=full-workflow-verification-in-run_pipeline-function>Full Workflow Verification in <code>run_pipeline</code> Function</h3><p>The <code>run_pipeline</code> function orchestrates the full workflow:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_pipeline</span>(self, user_message, temperature<span style=color:#f92672>=</span>GROQ_DEFAULT_TEMPERATURE, max_tokens<span style=color:#f92672>=</span>GROQ_DEFAULT_MAX_TOKENS, rounds<span style=color:#f92672>=</span>GROQ_DEFAULT_ROUNDS, multi_turn<span style=color:#f92672>=</span>GROQ_MULTITURN):
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;instruction&#34;</span>: [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(len(self<span style=color:#f92672>.</span>reference_models))],
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;model_info&#34;</span>: self<span style=color:#f92672>.</span>reference_models,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> multi_turn:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(self<span style=color:#f92672>.</span>reference_models)):
</span></span><span style=display:flex><span>            data[<span style=color:#e6db74>&#34;instruction&#34;</span>][i]<span style=color:#f92672>.</span>append({<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: user_message})
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        data[<span style=color:#e6db74>&#34;instruction&#34;</span>] <span style=color:#f92672>=</span> [[{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: user_message}]] <span style=color:#f92672>*</span> len(self<span style=color:#f92672>.</span>reference_models)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>messages<span style=color:#f92672>.</span>append({<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: user_message})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i_round <span style=color:#f92672>in</span> range(rounds):
</span></span><span style=display:flex><span>        logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Starting round </span><span style=color:#e6db74>{</span>i_round <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> of processing.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        responses <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>process_layer(data, temperature, max_tokens)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Responses after Round </span><span style=color:#e6db74>{</span>i_round <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>:&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i, response <span style=color:#f92672>in</span> enumerate(responses):
</span></span><span style=display:flex><span>            logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Model </span><span style=color:#e6db74>{</span>self<span style=color:#f92672>.</span>reference_models[i][<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>response[:<span style=color:#ae81ff>50</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>...&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;Aggregating results &amp; querying the aggregate model...&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    aggregated_responses <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>aggregate_responses(responses)
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>generate_with_references(
</span></span><span style=display:flex><span>        model_info<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>model_aggregate,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span>temperature,
</span></span><span style=display:flex><span>        max_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>        messages<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>messages,
</span></span><span style=display:flex><span>        references<span style=color:#f92672>=</span>responses,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final answer from </span><span style=color:#e6db74>{</span>self<span style=color:#f92672>.</span>model_aggregate[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;Output received from generate_with_references:&#34;</span>)
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(output)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> multi_turn:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(self<span style=color:#f92672>.</span>reference_models)):
</span></span><span style=display:flex><span>            data[<span style=color:#e6db74>&#34;instruction&#34;</span>][i]<span style=color:#f92672>.</span>append({<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;assistant&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: output})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>messages<span style=color:#f92672>.</span>append({<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;assistant&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: output})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> output
</span></span></code></pre></div><ul><li>The <code>run_pipeline</code> function sends the user message to the reference models.</li><li>Aggregates their responses.</li><li>Uses the aggregator model to synthesize the final response.</li><li>The synthesized response is returned and appended to the conversation history.</li></ul><h3 id=conclusion>Conclusion</h3><p>You can customer the number of reference models but it works best by sending a prompt to 3 reference models, aggregating their responses, and using an aggregator agent to synthesize a robust response.</p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=http://localhost:1313/>&copy; Ray Bernard's Fine-Tuned Journal 2024</a><div><div class=ananke-socials><a href=https://www.github.com/raymondbernard/fine-tune-journal target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="github link" aria-label="follow on github——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.linkedin.com/in/raymond-bernard-960382/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="My Linkedin link" aria-label="follow on My Linkedin——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q target=_blank rel=noopener class="youtube ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="youtube link" aria-label="follow on youtube——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527 41.34c-.278.0-.478.078-.6.244-.121.156-.18.424-.18.796v.896h1.543V42.38c0-.372-.062-.64-.185-.796C42.989 41.418 42.792 41.34 42.527 41.34zM36.509 41.309c.234.0.417.076.544.23.123.155.185.383.185.682v4.584c0 .286-.053.487-.153.611-.1.127-.256.189-.47.189-.148.0-.287-.033-.421-.096-.135-.062-.274-.171-.415-.313v-5.531c.119-.122.239-.213.36-.271C36.26 41.335 36.383 41.309 36.509 41.309zm5.239 3.349v1.672c0 .468.057.792.17.974.118.181.313.269.592.269.289.0.491-.076.606-.229.114-.153.175-.489.175-1.013v-.405h1.795v.456c0 .911-.217 1.596-.657 2.059-.435.459-1.089.687-1.958.687-.781.0-1.398-.242-1.847-.731-.448-.486-.676-1.157-.676-2.014v-3.986c0-.768.249-1.398.742-1.882.493-.484 1.128-.727 1.911-.727.799.0 1.413.225 1.843.674.429.448.642 1.093.642 1.935v2.264H41.748zm-3.125 3.837c-.271.336-.669.501-1.187.501-.343.0-.646-.062-.912-.192-.267-.129-.519-.327-.746-.601v.681h-1.764V36.852h1.764v3.875c.237-.27.485-.478.748-.616.267-.143.534-.212.805-.212.554.0.975.189 1.265.565.294.379.438.933.438 1.66v4.926C39.034 47.678 38.897 48.159 38.623 48.495zM30.958 48.884v-.976c-.325.361-.658.636-1.009.822-.349.191-.686.282-1.014.282-.405.0-.705-.129-.913-.396-.201-.266-.305-.658-.305-1.189v-7.422h1.744v6.809c0 .211.037.362.107.457.077.095.196.141.358.141.128.0.292-.062.488-.188.197-.125.375-.283.542-.475v-6.744H32.7v8.878H30.958zM24.916 38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916zm8.078-5.622c0-.001 12.08.018 13.514 1.45 1.439 1.435 1.455 8.514 1.455 8.555.0.0-.012 7.117-1.455 8.556C45.074 52.969 32.994 53 32.994 53s-12.079-.031-13.516-1.462c-1.438-1.435-1.441-8.502-1.441-8.556.0-.041.004-7.12 1.441-8.555 1.438-1.431 13.516-1.45 13.516-1.449zm9.526-3.723h-1.966v-1.08c-.358.397-.736.703-1.13.909-.392.208-.771.312-1.14.312-.458.0-.797-.146-1.027-.437-.229-.291-.345-.727-.345-1.311v-8.172h1.962v7.497c0 .231.045.399.127.502.08.104.216.156.399.156.143.0.327-.069.548-.206.22-.137.423-.312.605-.527v-7.422h1.966V29.255zM31.847 27.588c.139.147.339.219.6.219.266.0.476-.075.634-.223.157-.152.235-.358.235-.618v-5.327c0-.214-.08-.387-.241-.519-.16-.131-.37-.196-.628-.196-.241.0-.435.065-.586.196-.148.132-.225.305-.225.519v5.327C31.636 27.233 31.708 27.439 31.847 27.588zm-1.439-7.685c.528-.449 1.241-.674 2.132-.674.812.0 1.48.237 2.001.711.517.473.777 1.083.777 1.828v5.051c0 .836-.255 1.491-.762 1.968-.513.476-1.212.714-2.106.714-.858.0-1.547-.246-2.064-.736-.513-.492-.772-1.152-.772-1.983v-5.068C29.613 20.954 29.877 20.351 30.408 19.903zM24.262 16h-2.229l2.634 8.003v5.252h2.213v-5.5L29.454 16h-2.25l-1.366 5.298h-.139L24.262 16zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://x.com/raybernard007 target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://medium.com/@raybernard007 target=_blank rel=noopener class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="twitter link" aria-label="follow on twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 170 170" viewBox="0 0 170 170" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M46.5340803 65.2157554C46.6968378 63.6076572 46.0836 62.018231 44.8828198 60.93592L32.6512605 46.2010582V44H70.6302521l29.3557423 64.380952L125.794585 44H162v2.2010582L151.542017 56.2281011C150.640424 56.9153477 150.193188 58.0448862 150.380019 59.1628454V132.837155C150.193188 133.955114 150.640424 135.084652 151.542017 135.771899l10.213352 10.027043V148H110.38282V145.798942l10.580299-10.271605c1.039682-1.039389 1.039682-1.345091 1.039682-2.934744V73.0417402l-29.4169 74.7136978H88.6106443L54.3622782 73.0417402V123.115814C54.0767278 125.221069 54.7759199 127.3406 56.2581699 128.863022L70.0186741 145.55438V147.755438H31V145.55438l13.7605042-16.691358c1.4714579-1.524946 2.1298796-3.658537 1.7735761-5.747208V65.2157554z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span></a><a href=https://www.facebook.com/groups/1994768487609019 target=_blank rel=noopener class="facebook ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="reddit link" aria-label="follow on reddit——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg>
</span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>