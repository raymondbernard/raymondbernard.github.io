<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ray Bernard's Finetuned Journey</title><link>http://localhost:64941/</link><description>Recent content on Ray Bernard's Finetuned Journey</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:64941/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>http://localhost:64941/posts/llama3-updated/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:64941/posts/llama3-updated/</guid><description>Table of Contents Fine-tuning LLaMa 3 8B Instruct on Intel Max Series GPUs Step 1: Initial Setup Step 2: Check Intel XPU Availability and Retrieve Device Capabilities Step 3: Optimize Environment for Intel Max Series GPUs Step 4: Monitor XPU Memory Usage in Real-Time print(&amp;ldquo;XPU device not available.&amp;rdquo;) Step 5: Log into Your Hugging Face Account with Your Access Token Step 6: Configure LoRA for Efficient Training 🎛️ Step 7: Load and Prepare the Model Step 8: Testing the Model Step 9: Load and Inspect the Dataset Step 10: Format and Split the Dataset for Training Step 11: Fine-Tune the Model and Save the Results 📊 Step 12: Merge and Save the Fine-Tuned Model Step 13: Upload the Fine-Tuned Model to Hugging Face Hub 🚀 Step 14: Fine-Tuning Results and Observations Happy Fine-Tuning!</description></item></channel></rss>